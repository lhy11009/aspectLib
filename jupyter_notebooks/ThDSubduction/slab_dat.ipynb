{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "Download the utilities file at \n",
    "\n",
    "    https://github.com/lhy11009/Utilities/blob/master/python_scripts/Utilities.py\n",
    "\n",
    "and put it into a directory called\n",
    "\n",
    "    \"utilities\"\n",
    "\n",
    "Install the py-gplate package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the environment of py-gplate\n",
    "import sys\n",
    "import gplately\n",
    "import numpy as np\n",
    "import gplately.pygplates as pygplates\n",
    "from gplately import ptt\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import gridspec\n",
    "import cartopy.crs as ccrs\n",
    "from plate_model_manager import PlateModelManager\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "\n",
    "# directory to the aspect Lab\n",
    "# set the location of file outputs\n",
    "ASPECT_LAB_DIR = os.environ['ASPECT_LAB_DIR']\n",
    "base_dir = ASPECT_LAB_DIR\n",
    "RESULT_DIR = os.path.join(base_dir, 'results')\n",
    "Utilities_dir = os.path.join(base_dir, 'utilities', \"python_scripts\")\n",
    "if not os.path.isdir(Utilities_dir):\n",
    "    raise FileNotFoundError(\"The utilities Doesn't exist yet\")\n",
    "# import shilofue.GPlateLib as GPlateLib\n",
    "# import utilities in subdirectiory\n",
    "sys.path.append(Utilities_dir)\n",
    "import Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow for Subduction Dataset from Plate Reconstruction\n",
    "\n",
    "#### General workflow\n",
    "\n",
    "This workflow defines the steps to query a subduction plate dataset based on plate reconstruction data.\n",
    "\n",
    "##### Preliminary\n",
    "\n",
    "One must first use gplate and export a \"reconstruction\" file for the specific timestep. This file serves to look up names and ids of the subduction zones.\n",
    "\n",
    "##### Step 1: Loading Plate Reconstruction Data\n",
    "To extract additional properties such as **plate name**, **start time**, and **end time**, it is necessary to load the plate reconstruction files.\n",
    "\n",
    "##### Step 2: Fixing Subducting Plate Age\n",
    "To accurately determine the age of the subducting plate, we must ensure that the points are correctly pinned to the subducting plate. This involves two workflows:\n",
    "\n",
    "1. **Automatic Workflow**: The process attempts to automatically pin the points to the subducting plate.\n",
    "2. **Manual Workflow**: If the automatic workflow produces invalid values, the manual workflow is used to correct these errors.\n",
    "\n",
    "The results of both workflows are recorded in a CSV file, which can be loaded for further analysis.\n",
    "\n",
    "##### Step 3: Extracting Key Information\n",
    "The ultimate goal of the workflow is to pin sample points to the global subduction zones and extract the following information:\n",
    "\n",
    "- **Location of points**: Geographic coordinates of the sample points.\n",
    "- **Length of the arc**: The arc length of the subduction zone.\n",
    "- **Plate age**: The age of the subducting plate at the pinned location.\n",
    "- **Convergence rate**: The rate of convergence between the plates.\n",
    "- **Trench mobility**: The movement of the trench over time.\n",
    "- **Trench PID**: The process ID (PID) for the trench, as used in the reconstruction.\n",
    "- **Subduction PID**: The process ID (PID) for the subduction zone, as used in the reconstruction.\n",
    "\n",
    "> **Note**: The **trench PID** and **subduction PID** correspond to the IDs used in the plate reconstruction process.\n",
    "\n",
    "\n",
    "#### To get a good dataset for a time step\n",
    "\n",
    "1. **Select the Automatic Workflow and get a global dataset**:\n",
    "   - Use the global dataset with the automatic workflow to initiate the data processing. This is a good reference dataset to refine points of respective trenches\n",
    "\n",
    "\n",
    "2. **Look in the global dataset for a local subset or extract a Local dataset directly**:\n",
    "    - To directly extract a local dataset, look for a id for a trench. Put that in the workflow to extract a single trench\n",
    "    - When we are satisfied, damp the data in to a finalized dataset\n",
    "\n",
    "3. **Analyze the Finalzed Dataset** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions\n",
    "\n",
    "##### ReadFile Function\n",
    "\n",
    "###### Purpose\n",
    "The `ReadFile` function reads a file containing plate reconstruction data and extracts relevant subduction zone information. It identifies sections of the file corresponding to subduction zones, parses coordinates, and gathers metadata such as trench names, plate IDs, and valid time intervals.\n",
    "\n",
    "###### Parameters\n",
    "- **infile (str):** The file path to the input data file that contains the reconstruction information.\n",
    "\n",
    "###### Returns\n",
    "- **dict:** A dictionary with the following keys:\n",
    "  - **'n_trench' (int):** The total number of subduction zones found in the file.\n",
    "  - **'trench_data' (list):** A list of arrays, where each array contains the coordinates of a subduction zone.\n",
    "  - **'trench_names' (list):** A list of names associated with each subduction zone.\n",
    "  - **'trench_pids' (list):** A list of plate IDs corresponding to each subduction zone.\n",
    "  - **'trench_begin_times' (list):** A list of the beginning times for each subduction zone.\n",
    "  - **'trench_end_times' (list):** A list of the end times for each subduction zone.\n",
    "\n",
    "###### Implementation Overview\n",
    "- The function opens the input file and iterates through each line.\n",
    "- It uses flags (`sbd_begin` and `sbd_end`) to track the start and end of subduction zone sections.\n",
    "- Subduction zone coordinates and header metadata are extracted and stored in respective lists.\n",
    "- The function returns a dictionary containing the collected subduction zone data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadFile(infile):\n",
    "    \"\"\"\n",
    "    Reads a file of plate reconstruction data and extracts subduction zone information.\n",
    "    \n",
    "    Parameters:\n",
    "        infile (str): The file path to the input data file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - 'n_trench' (int): The number of subduction zones found in the file.\n",
    "            - 'trench_data' (list): A list of arrays containing coordinates for each subduction zone.\n",
    "            - 'trench_names' (list): A list of names for each subduction zone.\n",
    "            - 'trench_pids' (list): A list of plate IDs associated with each subduction zone.\n",
    "            - 'trench_begin_times' (list): A list of the beginning times for each subduction zone.\n",
    "            - 'trench_end_times' (list): A list of the end times for each subduction zone.\n",
    "    \n",
    "    Implementation:\n",
    "        - Opens the input file and iterates through each line.\n",
    "        - Uses flags `sbd_begin` and `sbd_end` to track whether the current section is a subduction zone.\n",
    "        - Extracts coordinates and header information, such as trench names, plate IDs, and time intervals.\n",
    "        - Appends the extracted data into respective lists and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "    infile = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \n",
    "                          \"Muller_etal_2019_PlateBoundaries_no_topologies\", \n",
    "                          \"reconstructed_0.00Ma.xy\")\n",
    "    assert(os.path.isfile(infile))\n",
    "\n",
    "    trench_data = []\n",
    "    trench_names = []\n",
    "    trench_pids = []\n",
    "    trench_begin_times = []\n",
    "    trench_end_times = []\n",
    "\n",
    "    i = 0\n",
    "    temp_l = []  # Stores line indices of each subduction zone section\n",
    "    temp_d = []  # Temporarily holds coordinates of the current subduction zone\n",
    "    n_trench = 0  # Counts the number of subduction zones\n",
    "    sbd_begin = False  # Flag indicating the start of a subduction zone section\n",
    "    sbd_end = False  # Flag indicating the end of a subduction zone section\n",
    "    read = True  # Flag for continuing to read the file\n",
    "\n",
    "    with open(infile, 'r') as fin:\n",
    "        line = fin.readline()\n",
    "        i += 1\n",
    "        while line:\n",
    "            read = True  # Default to continue reading each loop\n",
    "            \n",
    "            # Check if the end of a subduction zone section is reached\n",
    "            if sbd_begin and re.match('^>', line):\n",
    "                sbd_end = True\n",
    "\n",
    "            # Handle the different scenarios based on the flags\n",
    "            if sbd_begin and (not sbd_end):\n",
    "                # Reading subduction zone data\n",
    "                temp_data = line.split()\n",
    "                temp_data = [float(x) for x in temp_data]\n",
    "                temp_d.append(temp_data)\n",
    "            elif sbd_begin and sbd_end:\n",
    "                # Reached the end of a section, store the data and reset flags\n",
    "                trench_data.append(temp_d)\n",
    "                sbd_begin = False\n",
    "                sbd_end = False\n",
    "                read = False\n",
    "            elif re.match('^>SubductionZone', line):\n",
    "                # Found the start of a new subduction zone section\n",
    "                temp_l.append(i)\n",
    "                sbd_begin = True\n",
    "                temp_d = []\n",
    "                # Continue reading the headers of the section\n",
    "                while line and re.match('^>', line):\n",
    "                    line = fin.readline()\n",
    "                    i += 1\n",
    "                    if re.match('^> name', line):\n",
    "                        trench_names.append(Utilities.remove_substrings(line, [\"> name \", '\\n']))\n",
    "                    elif re.match('> reconstructionPlateId', line):\n",
    "                        trench_pids.append(int(Utilities.remove_substrings(line, [\"> reconstructionPlateId \", '\\n'])))\n",
    "                    elif re.match('> validTime TimePeriod <begin> TimeInstant <timePosition>', line):\n",
    "                        temp0 = Utilities.remove_substrings(line, [\"> validTime TimePeriod <begin> TimeInstant <timePosition>\", '</timePosition>.*\\n'])\n",
    "                        trench_begin_times.append(float(temp0))\n",
    "                        temp1 = Utilities.remove_substrings(line, ['^.*<end> TimeInstant <timePosition>', '</timePosition>.*\\n'])\n",
    "                        trench_end_times.append(float(temp1) if type(temp1) == float else 0.0)\n",
    "                read = False\n",
    "            \n",
    "            if read:\n",
    "                line = fin.readline()\n",
    "                i += 1\n",
    "\n",
    "    i -= 1  # Adjust for the last unsuccessful read\n",
    "    n_trench = len(trench_data)\n",
    "\n",
    "    outputs = {\n",
    "        \"n_trench\": n_trench, \n",
    "        \"trench_data\": trench_data, \n",
    "        \"trench_names\": trench_names,\n",
    "        \"trench_pids\": trench_pids, \n",
    "        \"trench_begin_times\": trench_begin_times, \n",
    "        \"trench_end_times\": trench_end_times\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def LookupNameByPid(trench_pids, trench_names, pid):\n",
    "    \"\"\"\n",
    "    Looks up the name of a trench using its plate ID.\n",
    "\n",
    "    Parameters:\n",
    "        trench_pids (list): A list of plate IDs corresponding to subduction zones.\n",
    "        trench_names (list): A list of names corresponding to the trench IDs.\n",
    "        pid (int): The plate ID for which the trench name is being looked up.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the trench corresponding to the given plate ID. Returns an empty\n",
    "             string if the plate ID is not found.\n",
    "    \n",
    "    Implementation:\n",
    "        - Asserts that the `pid` provided is of type `int`.\n",
    "        - Attempts to find the index of `pid` in the `trench_pids` list.\n",
    "        - If the `pid` is found, retrieves the name from `trench_names` using the index.\n",
    "        - If the `pid` is not found (raises a `ValueError`), returns an empty string.\n",
    "    \"\"\"\n",
    "    _name = \"\"\n",
    "    assert(type(pid) == int)\n",
    "    try:\n",
    "        _index = trench_pids.index(pid)\n",
    "    except ValueError:\n",
    "        _name = \"\"\n",
    "    else:\n",
    "        _name = trench_names[_index]\n",
    "    return _name\n",
    "\n",
    "\n",
    "def get_one_subduction_by_trench_id(subduction_data, trench_pid, all_columns):\n",
    "    \"\"\"\n",
    "    Extracts data for a specific subduction zone from the global dataset at a given reconstruction time.\n",
    "\n",
    "    Parameters:\n",
    "        subduction_data (list): The global dataset of subduction zones at a particular reconstruction time.\n",
    "                                Each element in this list represents a subduction zone and contains values\n",
    "                                corresponding to columns in `all_columns`.\n",
    "        trench_pid (int): The ID of the specific subduction zone to extract.\n",
    "        all_columns (list): The list of column names for the final DataFrame, containing 10 entries:\n",
    "                            ['lon', 'lat', 'conv_rate', 'conv_angle', 'trench_velocity', \n",
    "                             'trench_velocity_angle', 'arc_length', 'trench_azimuth_angle',\n",
    "                             'subducting_pid', 'trench_pid'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the data for the specified subduction zone, \n",
    "                      sorted by latitude (column index 1).\n",
    "    \n",
    "    Implementation:\n",
    "        - Initializes an empty list `ret` to store the selected subduction data.\n",
    "        - Iterates over `subduction_data` and appends rows that match the `trench_pid` at index 9.\n",
    "        - Sorts the collected rows by latitude (index 1) to order them spatially.\n",
    "        - Converts the sorted list into a pandas DataFrame using `all_columns` as headers.\n",
    "        - Returns the resulting DataFrame.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    Found = False\n",
    "    for row in subduction_data:\n",
    "        if row[9] == trench_pid:  # Only select rows where the trench ID matches\n",
    "            ret.append(row)\n",
    "            Found = True\n",
    "\n",
    "    # Assert the the given pid is contained in the subduction_data\n",
    "    assert(Found)\n",
    "    \n",
    "    # Sort the selected data by latitude (index 1)\n",
    "    ret.sort(key=lambda row: row[1])\n",
    "    \n",
    "    # Create a DataFrame with the selected and sorted data\n",
    "    one_subduction_data = pd.DataFrame(ret, columns=all_columns)\n",
    "    \n",
    "    return one_subduction_data\n",
    "\n",
    "\n",
    "def plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time):\n",
    "    \"\"\"\n",
    "    Plots basic global geological features on a given axis, including coastlines and an age grid.\n",
    "\n",
    "    Parameters:\n",
    "        ax (matplotlib.axes._axes.Axes): The axis on which to plot the global features.\n",
    "        gplot (gplately.plot.PlotTopologies): An object used for plotting geological features such as coastlines.\n",
    "        age_grid_raster: A raster object containing age data, typically used for visualizing geological ages.\n",
    "        reconstruction_time (float): The geological time at which the reconstruction is plotted.\n",
    "    \n",
    "    Implementation:\n",
    "        - Configures global gridlines on the plot with specific color, linestyle, and locations.\n",
    "        - Sets the map extent to global.\n",
    "        - Uses the `gplot` object to plot coastlines at the given reconstruction time.\n",
    "        - Plots the age grid data on the map using a specified colormap and transparency level.\n",
    "        - Adds a color bar to the plot to represent ages, with a labeled color bar axis.\n",
    "    \"\"\"\n",
    "    # Configure global gridlines with specified color and linestyle\n",
    "    gl = ax.gridlines(color='0.7', linestyle='--', xlocs=np.arange(-180, 180, 15), ylocs=np.arange(-90, 90, 15))\n",
    "    gl.left_labels = True\n",
    "\n",
    "    # Set the map extent to global\n",
    "    ax.set_global()\n",
    "\n",
    "    # Set the reconstruction time for the gplot object and plot coastlines in grey\n",
    "    gplot.time = reconstruction_time\n",
    "    gplot.plot_coastlines(ax, color='grey')\n",
    "\n",
    "    # Plot the age grid on the map using a colormap from yellow to blue\n",
    "    im_age = gplot.plot_grid(ax, age_grid_raster.data, cmap='YlGnBu', vmin=0, vmax=200, alpha=0.8)\n",
    "\n",
    "    # Add a color bar for the age grid with a label\n",
    "    cbar_age = plt.colorbar(im_age)\n",
    "    cbar_age.ax.get_yaxis().labelpad = 15\n",
    "    cbar_age.ax.set_ylabel(\"Age (Ma)\", rotation=90)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def resample_subduction(one_subduction_data, arc_length_edge, arc_length_resample_section, all_columns, **kwargs):\n",
    "    \"\"\"\n",
    "    Resamples data points from a dense subduction zone at specified intervals along its arc length.\n",
    "    This helps simplify and extract key properties of the subduction zone for plotting and analysis.\n",
    "\n",
    "    Parameters:\n",
    "        one_subduction_data (pd.DataFrame): A pandas DataFrame containing data for a single subduction zone.\n",
    "        arc_length_edge (float): The arc length distance from the edges where no resampling is performed.\n",
    "        arc_length_resample_section (float): The interval at which the arc length is resampled.\n",
    "        all_columns (list): A list of column names for the output DataFrame.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "            - indent (int, optional): Indentation for the output log content. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - pd.DataFrame: A DataFrame of the resampled subduction zone data.\n",
    "            - str: A log of the resampled points' coordinates for debugging or output purposes.\n",
    "\n",
    "    Implementation:\n",
    "        - Initializes variables, including indentation and a log for output content.\n",
    "        - Computes cumulative arc lengths for all points in the original data.\n",
    "        - Determines resampling points centered at the midpoint of the arc length and propagates outward.\n",
    "        - Resamples properties by linear interpolation between points, including special handling of longitude and latitude.\n",
    "        - Collects and logs each resampled point's coordinates, and returns the resampled DataFrame and the log.\n",
    "    \"\"\"\n",
    "    # Initialize variables, including default indentation for output\n",
    "    indent = kwargs.get(\"indent\", 0)  # Default is no indentation\n",
    "    log_output_contents = \"\"\n",
    "    data_len = len(one_subduction_data)\n",
    "    \n",
    "    # Compute cumulative arc lengths\n",
    "    arc_lengths = one_subduction_data['arc_length']\n",
    "    arc_length_sums = np.zeros(data_len)\n",
    "    arc_length_sums[0] = arc_lengths[0]\n",
    "    for i in range(1, data_len):\n",
    "        arc_length_sums[i] = arc_length_sums[i - 1] + arc_lengths[i]\n",
    "\n",
    "    # Compute resampling points: start at the center and propagate outward\n",
    "    temp = []\n",
    "    if arc_length_sums[-1] > 2 * arc_length_edge:\n",
    "        temp.append(arc_length_sums[-1] / 2.0)\n",
    "    i = 1\n",
    "    arc_length_sum_temp = arc_length_sums[-1] / 2.0 - arc_length_resample_section / 2.0\n",
    "    arc_length_sum_temp1 = arc_length_sums[-1] / 2.0 + arc_length_resample_section / 2.0\n",
    "    while arc_length_sum_temp > arc_length_edge:\n",
    "        temp.append(arc_length_sum_temp)\n",
    "        temp.append(arc_length_sum_temp1)\n",
    "        arc_length_sum_temp -= arc_length_resample_section\n",
    "        arc_length_sum_temp1 += arc_length_resample_section\n",
    "    arc_length_sums_resampled = sorted(temp)\n",
    "\n",
    "    # Resample properties of the subduction zone by interpolation\n",
    "    one_subduction_data_resampled = pd.DataFrame(columns=all_columns)\n",
    "    i_sbd_re = 0\n",
    "    is_first = True\n",
    "    for arc_length_sum_resampled in arc_length_sums_resampled:\n",
    "        for i in range(len(arc_length_sums) - 1):\n",
    "            if (arc_length_sums[i] <= arc_length_sum_resampled) and (arc_length_sum_resampled < arc_length_sums[i + 1]):\n",
    "                # Calculate the interpolation fraction\n",
    "                fraction = (arc_length_sum_resampled - arc_length_sums[i]) / (arc_length_sums[i + 1] - arc_length_sums[i])\n",
    "                row_temp = fraction * one_subduction_data.iloc[i] + (1. - fraction) * one_subduction_data.iloc[i + 1]\n",
    "                \n",
    "                # Interpolate longitude and latitude using a custom mapping method\n",
    "                row_temp.loc[\"lon\"], row_temp.loc[\"lat\"] = Utilities.map_mid_point(\n",
    "                    one_subduction_data.iloc[i].lon, one_subduction_data.iloc[i].lat,\n",
    "                    one_subduction_data.iloc[i + 1].lon, one_subduction_data.iloc[i + 1].lat, fraction\n",
    "                )\n",
    "\n",
    "                # Log the resampled point's coordinates\n",
    "                log_output_contents += \"%s%d th resampled point: (%.2f, %.2f)\\n\" % (\" \" * indent, i_sbd_re, row_temp.lon, row_temp.lat)\n",
    "                \n",
    "                # Append the interpolated row to the resampled DataFrame\n",
    "                if is_first:\n",
    "                    one_subduction_data_resampled = pd.DataFrame([row_temp])\n",
    "                    is_first = False\n",
    "                else:\n",
    "                    one_subduction_data_resampled = pd.concat([one_subduction_data_resampled, pd.DataFrame([row_temp])], ignore_index=True)\n",
    "        i_sbd_re += 1\n",
    "\n",
    "    return one_subduction_data_resampled, log_output_contents\n",
    "\n",
    "\n",
    "def ResampleAllSubduction(subduction_data, trench_pids, arc_length_edge, arc_length_resample_section, all_columns):\n",
    "    \"\"\"\n",
    "    Resamples all specified subduction zones in the dataset by their trench plate IDs.\n",
    "\n",
    "    Parameters:\n",
    "        subduction_data (pd.DataFrame): The global dataset of subduction zones at a reconstruction time.\n",
    "        trench_pids (list): A list of trench plate IDs for which to perform resampling.\n",
    "        arc_length_edge (float): The arc length distance from the edges where no resampling is performed.\n",
    "        arc_length_resample_section (float): The interval at which the arc length is resampled.\n",
    "        all_columns (list): A list of column names for the final resampled DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the resampled data for all specified subduction zones.\n",
    "    \n",
    "    Implementation:\n",
    "        - Initializes an empty DataFrame and a string to collect log output.\n",
    "        - Iterates through each `trench_pid` in `trench_pids`:\n",
    "            - Extracts data for the subduction zone with `trench_pid`.\n",
    "            - Calls `resample_subduction` to resample the data, logging the process.\n",
    "            - Appends the resampled data to the overall DataFrame.\n",
    "        - Concatenates all resampled subduction data into a single DataFrame and returns it.\n",
    "    \"\"\"\n",
    "    subduction_data_resampled = None\n",
    "    log_output_contents = \"\"\n",
    "\n",
    "    # Iterate over each trench plate ID and resample the subduction zone data\n",
    "    for i in range(len(trench_pids)):\n",
    "        trench_pid = trench_pids[i]\n",
    "        \n",
    "        # Extract data for the current subduction zone\n",
    "        one_subduction_data = get_one_subduction_by_trench_id(subduction_data, trench_pid, all_columns)\n",
    "        \n",
    "        # Resample the subduction data and collect log output\n",
    "        one_subduction_data_resampled, log_output_contents = resample_subduction(\n",
    "            one_subduction_data, arc_length_edge, arc_length_resample_section, all_columns, indent=4\n",
    "        )\n",
    "\n",
    "        # Add information about the start and end points of the subduction zone\n",
    "        log_output_contents += \"%d th arc\\n\" % i\n",
    "        log_output_contents += \"start (%.2f, %.2f)\\n\" % (one_subduction_data.iloc[0].lon, one_subduction_data.iloc[0].lat)\n",
    "        log_output_contents += \"end (%.2f, %.2f)\\n\" % (one_subduction_data.iloc[-1].lon, one_subduction_data.iloc[-1].lat)\n",
    "\n",
    "        # Initialize or concatenate the resampled data into the main DataFrame\n",
    "        if i == 0:\n",
    "            subduction_data_resampled = one_subduction_data_resampled\n",
    "        else:\n",
    "            if not one_subduction_data_resampled.empty:\n",
    "                subduction_data_resampled = pd.concat(\n",
    "                    [subduction_data_resampled, one_subduction_data_resampled], ignore_index=True\n",
    "                )\n",
    "\n",
    "    subduction_data_resampled = pd.DataFrame(subduction_data_resampled, columns=all_columns)\n",
    "\n",
    "    return subduction_data_resampled\n",
    "\n",
    "\n",
    "def ResampleSubductionById(subduction_data, trench_pid, arc_length_edge, arc_length_resample_section, all_columns):\n",
    "    \"\"\"\n",
    "    Resamples a specific subduction zone from the global dataset using its trench plate ID.\n",
    "\n",
    "    Parameters:\n",
    "        subduction_data (pd.DataFrame): The global dataset of subduction zones at a reconstruction time.\n",
    "        trench_pid (int): The ID of the trench subduction zone to resample.\n",
    "        arc_length_edge (float): The arc length distance from the edges where no resampling is performed.\n",
    "        arc_length_resample_section (float): The interval at which the arc length is resampled.\n",
    "        all_columns (list): A list of column names for the final resampled DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: A DataFrame of the resampled subduction zone data.\n",
    "            - str: A log of the resampled points' details for debugging or output purposes.\n",
    "    \n",
    "    Implementation:\n",
    "        - Extracts data for the subduction zone using `get_one_subduction_by_trench_id`.\n",
    "        - Calls `resample_subduction` to resample the data based on the provided parameters.\n",
    "        - Returns the resampled DataFrame and the log output string.\n",
    "    \"\"\"\n",
    "    # Extract data for the specified subduction zone using the trench plate ID\n",
    "    one_subduction_data = get_one_subduction_by_trench_id(subduction_data, trench_pid)\n",
    "\n",
    "    # Resample the subduction zone data and get the log output\n",
    "    one_subduction_data_resampled, log_output_contents = resample_subduction(\n",
    "        one_subduction_data, arc_length_edge, arc_length_resample_section, all_columns, indent=4\n",
    "    )\n",
    "\n",
    "    return one_subduction_data_resampled, log_output_contents\n",
    "\n",
    "\n",
    "def FixTrenchAgeLocal(subduction_data, age_grid_raster, i_p, theta):\n",
    "    \"\"\"\n",
    "    Fixes invalid age values in a subduction data object using age interpolation\n",
    "    from nearby points along a specified direction.\n",
    "\n",
    "    Parameters:\n",
    "        subduction_data (pd.DataFrame): The dataset containing subduction zone data.\n",
    "        age_grid_raster: A raster object containing age data, typically used for visualizing geological ages.\n",
    "        i_p (int): The index of the subduction data point to be fixed.\n",
    "        theta (float): The direction (in degrees) to search for new data points for interpolation.\n",
    "\n",
    "    Returns:\n",
    "        float: The newly interpolated age value. If interpolation fails, returns NaN.\n",
    "    \n",
    "    Implementation:\n",
    "        - Defines a set of distances `ds` to search for new points around the specified index.\n",
    "        - Iterates over pairs of distances to generate two nearby points in the specified direction.\n",
    "        - Uses `Utilities.map_point_by_distance` to calculate the longitude and latitude of the new points.\n",
    "        - If both ages are valid, interpolates between them to determine the new age.\n",
    "        - Updates the `subduction_data` object with the interpolated age and records the fixed location.\n",
    "        - If interpolation is not successful, sets the age to NaN.\n",
    "    \"\"\"\n",
    "    ds = [12.5e3, 25e3, 50e3, 75e3, 100e3, 150e3, 200e3, 300e3, 400e3]\n",
    "    new_age = np.nan\n",
    "\n",
    "    # Iterate over the distances to generate two points for age interpolation\n",
    "    for j in range(len(ds) - 1):\n",
    "        # Generate two local points at distances `ds[j]` and `ds[j+1]` in the direction `theta`\n",
    "        subduction_data_local0 = pd.DataFrame([subduction_data.iloc[i_p]])\n",
    "        subduction_data_local1 = pd.DataFrame([subduction_data.iloc[i_p]])\n",
    "        \n",
    "        subduction_data_local0.loc[:, \"lon\"], subduction_data_local0.loc[:, \"lat\"] = Utilities.map_point_by_distance(\n",
    "            subduction_data.iloc[i_p].lon, subduction_data.iloc[i_p].lat, theta, ds[j]\n",
    "        )\n",
    "        subduction_data_local1.loc[:, \"lon\"], subduction_data_local1.loc[:, \"lat\"] = Utilities.map_point_by_distance(\n",
    "            subduction_data.iloc[i_p].lon, subduction_data.iloc[i_p].lat, theta, ds[j + 1]\n",
    "        )\n",
    "        \n",
    "        # Interpolate ages at the two new points\n",
    "        new_age0 = age_grid_raster.interpolate(subduction_data_local0.lon, subduction_data_local0.lat, method=\"nearest\")\n",
    "        new_age1 = age_grid_raster.interpolate(subduction_data_local1.lon, subduction_data_local1.lat, method=\"nearest\")\n",
    "        \n",
    "        # If both ages are valid, perform interpolation and update the subduction data\n",
    "        if (not np.isnan(new_age0)) and (not np.isnan(new_age1)):\n",
    "            new_age = (new_age0 * ds[j + 1] - new_age1 * ds[j]) / (ds[j + 1] - ds[j])\n",
    "            subduction_data.loc[i_p, \"age\"] = new_age\n",
    "            # debug\n",
    "            subduction_data.loc[i_p, \"lon_fix\"] = subduction_data_local1.lon.iloc[0]  # Records the further point\n",
    "            subduction_data.loc[i_p, \"lat_fix\"] = subduction_data_local0.lat.iloc[0]  # Records the closer point\n",
    "            break\n",
    "        else:\n",
    "            subduction_data.loc[i_p, \"age\"] = np.nan  # Mark as NaN if interpolation fails\n",
    "\n",
    "    return new_age\n",
    "\n",
    "\n",
    "def FixTrenchAge(subduction_data, age_grid_raster, **kwargs):\n",
    "    '''\n",
    "    Fix the trench ages in subduction_data\n",
    "    Inputs:\n",
    "        subduction_data: pandas object, subduction dataset\n",
    "        age_grid_raster: A raster object containing age data, typically used for visualizing geological ages.\n",
    "    '''\n",
    "    # automatically fix the invalid ages \n",
    "    for i in range(len(subduction_data)):\n",
    "        fix_age_polarity = subduction_data.fix_age_polarity[i]\n",
    "        if not np.isnan(fix_age_polarity):\n",
    "            # fix with existing polarity\n",
    "            # 0 and 1: on different side of the trench\n",
    "            # 2: manually assign values of longitude and latitude\n",
    "            if (fix_age_polarity == 0): \n",
    "                new_age = FixTrenchAgeLocal(subduction_data, age_grid_raster, i, subduction_data.trench_azimuth_angle[i] + 180.0)\n",
    "            elif (fix_age_polarity == 1): \n",
    "                new_age = FixTrenchAgeLocal(subduction_data, age_grid_raster, i, subduction_data.trench_azimuth_angle[i])\n",
    "            elif (fix_age_polarity == 2):\n",
    "                subduction_data_local0 = pd.DataFrame([subduction_data.iloc[i]])\n",
    "                subduction_data_local0.loc[:, \"lon\"], subduction_data_local0.loc[:, \"lat\"] = subduction_data.iloc[i].lon_fix, subduction_data.iloc[i].lat_fix\n",
    "                new_age = age_grid_raster.interpolate(subduction_data_local0.lon, subduction_data_local0.lat, method=\"nearest\")\n",
    "                subduction_data.loc[i, 'age'] = new_age\n",
    "                pass\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            # figure out a possible polarity\n",
    "            new_age = FixTrenchAgeLocal(subduction_data, age_grid_raster, i, subduction_data.trench_azimuth_angle[i] + 180.0)\n",
    "            if np.isnan(new_age):\n",
    "                # next, try the other direction\n",
    "                new_age = FixTrenchAgeLocal(subduction_data, age_grid_raster, i, subduction_data.trench_azimuth_angle[i])\n",
    "                if not np.isnan(new_age):\n",
    "                    subduction_data.loc[i, \"fix_age_polarity\"] = 1\n",
    "            else:\n",
    "                subduction_data.loc[i, \"fix_age_polarity\"] = 0\n",
    "\n",
    "\n",
    "def MaskBySubductionTrenchIds(subduction_data, subducting_pid, trench_pid, i_p):\n",
    "    \"\"\"\n",
    "    Generates a combined mask for subduction data based on user selection or specific \n",
    "    subducting and trench IDs.\n",
    "    \n",
    "    Parameters:\n",
    "        subduction_data (pd.DataFrame): The DataFrame containing subduction data to be filtered.\n",
    "        subducting_pid (int or None): The subducting plate ID to match. If None, all IDs are included.\n",
    "        trench_pid (int or None): The trench plate ID to match. If None, all IDs are included.\n",
    "        i_p (list or None): List of indices selected by the user. If not None, these indices are used.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A boolean mask combining the specified conditions for filtering the data.\n",
    "    \n",
    "    Implementation:\n",
    "        - If `i_p` is provided, create `mask1` to select only those indices.\n",
    "        - If `subducting_pid` is provided, create `mask1` to select rows matching the `subducting_pid`.\n",
    "        - If neither is provided, `mask1` includes all rows.\n",
    "        - If `trench_pid` is provided, create `mask2` to select rows matching the `trench_pid`.\n",
    "        - If `trench_pid` is not provided, `mask2` includes all rows.\n",
    "        - The final mask is the logical AND of `mask1` and `mask2`.\n",
    "    \"\"\"\n",
    "    if i_p is not None:\n",
    "        mask1 = np.zeros(len(subduction_data), dtype=bool)\n",
    "        mask1[i_p] = 1\n",
    "    elif subducting_pid is not None:\n",
    "        # Generate mask1 based on the provided subducting plate ID\n",
    "        mask1 = subduction_data.subducting_pid == subducting_pid\n",
    "    else:\n",
    "        mask1 = np.ones(len(subduction_data), dtype=bool)\n",
    "\n",
    "    if trench_pid is not None:\n",
    "        # Generate mask2 based on the provided trench plate ID\n",
    "        mask2 = subduction_data.trench_pid == trench_pid\n",
    "    else:\n",
    "        mask2 = np.ones(len(subduction_data), dtype=bool)\n",
    "\n",
    "    return (mask1 & mask2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "At first run, run these tests by setting run_tests = True.\n",
    "Afterward, set this option to False to skip the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: debug\n",
    "\n",
    "run_tests = False\n",
    "\n",
    "def test_name_lookup_read_file():\n",
    "    \"\"\"\n",
    "    Tests the `ReadFile` function to ensure it correctly reads and parses subduction zone data.\n",
    "    \n",
    "    Implementation:\n",
    "        - Loads the subduction name lookup file from a specified path.\n",
    "        - Calls the `ReadFile` function with the provided file path.\n",
    "        - Asserts that the number of subduction zones (`n_trench`) is as expected.\n",
    "        - Asserts that the plate ID 201 appears the expected number of times in the list of trench plate IDs.\n",
    "    \n",
    "    Assertions:\n",
    "        - The test checks if `n_trench` equals 52, confirming the total count of trenches.\n",
    "        - The test ensures that the plate ID 201 appears 7 times, validating the parsing of plate IDs.\n",
    "    \"\"\"\n",
    "    subduction_name_lookup_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \n",
    "                                               \"Muller_etal_2019_PlateBoundaries_no_topologies\", \n",
    "                                               \"reconstructed_0.00Ma.xy\")\n",
    "\n",
    "    outputs = ReadFile(subduction_name_lookup_file)\n",
    "    assert(outputs['n_trench'] == 52)\n",
    "    assert(outputs[\"trench_pids\"].count(201) == 7)\n",
    "\n",
    "def test_main_workflow():\n",
    "\n",
    "    # assign a reconstruction time\n",
    "    reconstruction_time=0 # time of reconstruction, must be integar\n",
    "\n",
    "    # enter the directory of the plate reconstruction files\n",
    "    # dir_re = os.path.join(ASPECT_LAB_DIR, \"dtemp/gplate_export_test0/Muller_etal_2019_PlateBoundaries_no_topologies\")\n",
    "    # subduction_name_lookup_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"Muller_etal_2019_PlateBoundaries_no_topologies\", \"reconstructed_0.00Ma.xy\")\n",
    "\n",
    "    # fact checks\n",
    "    assert(type(reconstruction_time) == int)\n",
    "    # assert(os.path.isdir(dir_re))\n",
    "\n",
    "    # Initiation\n",
    "    # Initialize the anchor plate ID for the reconstruction model\n",
    "    anchor_plate_id = 0\n",
    "\n",
    "    # Define the columns used in the subduction data DataFrame\n",
    "    all_columns = ['lon', 'lat', 'conv_rate', 'conv_angle', 'trench_velocity', \n",
    "                            'trench_velocity_angle', 'arc_length', 'trench_azimuth_angle', \n",
    "                            'subducting_pid', 'trench_pid']\n",
    "\n",
    "    # Create an instance of the PlateModelManager to manage plate models\n",
    "    pm_manager = PlateModelManager()\n",
    "\n",
    "    # Load the \"Muller2019\" plate model from the specified data directory\n",
    "    plate_model = pm_manager.get_model(\"Muller2019\", data_dir=\"plate-model-repo\")\n",
    "\n",
    "    # Set up the PlateReconstruction model using the loaded plate model data\n",
    "    # This includes rotation models, topologies, and static polygons, with the specified anchor plate ID\n",
    "    model = gplately.PlateReconstruction(\n",
    "        plate_model.get_rotation_model(), \n",
    "        plate_model.get_topologies(), \n",
    "        plate_model.get_static_polygons(),\n",
    "        anchor_plate_id=anchor_plate_id\n",
    "    )\n",
    "\n",
    "    # Initialize the plotting object for visualizing topologies\n",
    "    # The layers used for plotting include coastlines, continental polygons, and COBs (Continental Ocean Boundaries)\n",
    "    gplot = gplately.plot.PlotTopologies(\n",
    "        model, \n",
    "        plate_model.get_layer('Coastlines'), \n",
    "        plate_model.get_layer('ContinentalPolygons'), \n",
    "        plate_model.get_layer('COBs')\n",
    "    )\n",
    "\n",
    "    # Initialize the reconstruction time at 0 (current time)\n",
    "    reconstruction_time = 0\n",
    "\n",
    "    # Initialize variables to hold subduction data and trench IDs\n",
    "    subduction_data = None\n",
    "\n",
    "    # Initialize the age grid raster, which will be used for age-related computations\n",
    "    age_grid_raster = None\n",
    "\n",
    "    # get the reconstruction of subduction zones\n",
    "    subduction_data = model.tessellate_subduction_zones(reconstruction_time, \n",
    "                                                    # tessellation_threshold_radians=0.01, \n",
    "                                                        anchor_plate_id=anchor_plate_id,\n",
    "                                                        ignore_warnings=True)\n",
    "    # get all the trench ids\n",
    "    temp = [row[9] for row in subduction_data]\n",
    "    trench_pids = sorted(set(temp))\n",
    "\n",
    "    # get the age grid raster\n",
    "    age_grid_raster = gplately.Raster(\n",
    "                                    data=plate_model.get_raster(\"AgeGrids\",reconstruction_time),\n",
    "                                    plate_reconstruction=model,\n",
    "                                    extent=[-180, 180, -90, 90]\n",
    "                                    )\n",
    "\n",
    "    arc_length_edge = 2.0\n",
    "    arc_length_resample_section = 2.0\n",
    "\n",
    "    subduction_data_resampled = ResampleAllSubduction(subduction_data, trench_pids, arc_length_edge, arc_length_resample_section, all_columns)\n",
    "    \n",
    "    subduction_data_resampled.loc[:, 'age'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'lon_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'lat_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'fix_age_polarity'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    # # todo_ages\n",
    "    subduction_data_resampled.loc[:, 'marker'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'marker_fill'] = ['none' for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'color'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "\n",
    "    assert(\"trench_azimuth_angle\" in subduction_data_resampled.columns and \"arc_length\" in subduction_data_resampled.columns)\n",
    "\n",
    "    # fix the ages\n",
    "    FixTrenchAge(subduction_data_resampled, age_grid_raster)\n",
    "\n",
    "\n",
    "if run_tests:\n",
    "    test_name_lookup_read_file()\n",
    "    test_main_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "First assign a reconstruction time\n",
    "\n",
    "Here I need a file to lookup ids and names of the subductions for each reconstruction time.\n",
    "\n",
    "#### Parameters\n",
    "* reconstruction_time - time of reconstruction\n",
    "* subduction_name_lookup_file - file export from gplate. This is needed to look up for the names of subduction zones and their ids. Gplately doesn't seem to preserve this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a reconstruction time\n",
    "reconstruction_time=0 # time of reconstruction, must be integar\n",
    "\n",
    "# enter the directory of the plate reconstruction files\n",
    "dir_re = os.path.join(ASPECT_LAB_DIR, \"dtemp/gplate_export_test0/Muller_etal_2019_PlateBoundaries_no_topologies\")\n",
    "subduction_name_lookup_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"Muller_etal_2019_PlateBoundaries_no_topologies\", \"reconstructed_0.00Ma.xy\")\n",
    "\n",
    "# fact checks\n",
    "assert(type(reconstruction_time) == int)\n",
    "assert(os.path.isdir(dir_re))\n",
    "\n",
    "# Initiation\n",
    "# Initialize the anchor plate ID for the reconstruction model\n",
    "anchor_plate_id = 0\n",
    "\n",
    "# Define the columns used in the subduction data DataFrame\n",
    "all_columns = ['lon', 'lat', 'conv_rate', 'conv_angle', 'trench_velocity', \n",
    "                          'trench_velocity_angle', 'arc_length', 'trench_azimuth_angle', \n",
    "                          'subducting_pid', 'trench_pid']\n",
    "\n",
    "# Create an instance of the PlateModelManager to manage plate models\n",
    "pm_manager = PlateModelManager()\n",
    "\n",
    "# Load the \"Muller2019\" plate model from the specified data directory\n",
    "plate_model = pm_manager.get_model(\"Muller2019\", data_dir=\"plate-model-repo\")\n",
    "\n",
    "# Set up the PlateReconstruction model using the loaded plate model data\n",
    "# This includes rotation models, topologies, and static polygons, with the specified anchor plate ID\n",
    "model = gplately.PlateReconstruction(\n",
    "    plate_model.get_rotation_model(), \n",
    "    plate_model.get_topologies(), \n",
    "    plate_model.get_static_polygons(),\n",
    "    anchor_plate_id=anchor_plate_id\n",
    ")\n",
    "\n",
    "# Initialize the plotting object for visualizing topologies\n",
    "# The layers used for plotting include coastlines, continental polygons, and COBs (Continental Ocean Boundaries)\n",
    "gplot = gplately.plot.PlotTopologies(\n",
    "    model, \n",
    "    plate_model.get_layer('Coastlines'), \n",
    "    plate_model.get_layer('ContinentalPolygons'), \n",
    "    plate_model.get_layer('COBs')\n",
    ")\n",
    "\n",
    "# Initialize the reconstruction time at 0 (current time)\n",
    "reconstruction_time = 0\n",
    "\n",
    "# Initialize variables to hold subduction data and trench IDs\n",
    "subduction_data = None\n",
    "\n",
    "# Initialize the age grid raster, which will be used for age-related computations\n",
    "age_grid_raster = None\n",
    "\n",
    "# get the reconstruction of subduction zones\n",
    "subduction_data = model.tessellate_subduction_zones(reconstruction_time, \n",
    "                                                   # tessellation_threshold_radians=0.01, \n",
    "                                                    anchor_plate_id=anchor_plate_id,\n",
    "                                                    ignore_warnings=True)\n",
    "# get all the trench ids\n",
    "temp = [row[9] for row in subduction_data]\n",
    "trench_pids = sorted(set(temp))\n",
    "\n",
    "# get the age grid raster\n",
    "age_grid_raster = gplately.Raster(\n",
    "                                data=plate_model.get_raster(\"AgeGrids\",reconstruction_time),\n",
    "                                plate_reconstruction=model,\n",
    "                                extent=[-180, 180, -90, 90]\n",
    "                                )\n",
    "\n",
    "\n",
    "name_lookups = ReadFile(subduction_name_lookup_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search a Single Subduction Zone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search with a key word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(name_lookups[\"trench_names\"])\n",
    "\n",
    "# keyword = \"ryu\"\n",
    "\n",
    "# matching_indices = [i for i, name in enumerate(name_lookups[\"trench_names\"]) if keyword.lower() in name.lower()]\n",
    "# for index in matching_indices:\n",
    "#     print(index)\n",
    "#     print(\"name: \", name_lookups[\"trench_names\"][index])\n",
    "#     print(\"id: \", name_lookups[\"trench_pids\"][index])\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search with a trench id\n",
    "\n",
    "The following block creates a global map displaying geological features and highlights a specific subduction\n",
    "In order to do this, it first looks up in a global dataset. And then plot the one subduction zone in a global map\n",
    "\n",
    "##### trench id, t = 0\n",
    "* 12001, ,CAS\n",
    "* 686, Indonesian bndy w AUS-mg, ANDA-SUM\n",
    "* 736, Java SZ\n",
    "* 651, Flores Banda SZ, JAVA\n",
    "* 669, North Sulawesi Subduction, SULA\n",
    "* 612, Luzon subduction, LUZ\n",
    "* 678, Philippine trench, PHIL\n",
    "* 648, Okinawa Trough (Ryuku) from EarthByte cob MG 4-20-07\n",
    "* 659, Izu Bonin Trench\n",
    "* 699, Marianas Trench-NUVEL\n",
    "* 111, Aleutian and Bering Sea Masking Polygon\n",
    "* 406, Kamchatka SZ from EarthByte COB at 0Ma -- MG 4/20/07\n",
    "* 413, Shirshov Ridge Subduction (not included), part of 901 (subduction id)\n",
    "* 2000, Central American subduction from 135 Ma\n",
    "* 201, South America trench taken from a combination of RUM and COB file\n",
    "* 2031, Caribbean/farallon subduction iniating at 85 Ma\n",
    "* 2011, Caribbean subduction\n",
    "* 815, Sandwich Trench\n",
    "* 821, Tonga-Kermadec-MG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: the current plot functions result in a redundant grid around the actual plot\n",
    "# # Define the trench plate ID\n",
    "# trench_pid = 111\n",
    "# # Look up the name of the subduction zone using the trench ID\n",
    "# _name = LookupNameByPid(name_lookups[\"trench_pids\"], name_lookups[\"trench_names\"], trench_pid)\n",
    "# print(_name)\n",
    "\n",
    "# # Get the data for the specified subduction zone using the trench ID\n",
    "# # The data is returned as a pandas DataFrame\n",
    "# one_subduction_data = get_one_subduction_by_trench_id(subduction_data, trench_pid, all_columns)\n",
    "\n",
    "# # Create a new figure for plotting\n",
    "# fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "# plt.title(f'{reconstruction_time} Ma')  # Set the title to the reconstruction time\n",
    "\n",
    "# # Set up the axes with a Mollweide projection, centered at longitude 180\n",
    "# ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude=180))\n",
    "\n",
    "# # Plot global features such as coastlines and the age grid using the custom plotting function\n",
    "# ax = plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "# # Plot the subduction zone data as red scatter points\n",
    "# # Use the PlateCarree projection to correctly position the points on the map\n",
    "# im_sub = ax.scatter(one_subduction_data.lon, one_subduction_data.lat, \n",
    "#                     marker=\".\", s=3, c='r', transform=ccrs.PlateCarree())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow to extract a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Methodology\n",
    "\n",
    "In this section, we define the methodology for working with the subduction dataset.\n",
    "\n",
    "1. **Loading Data**: \n",
    "   - You can choose to either load an existing CSV file containing previously saved data or start the process from scratch.\n",
    "\n",
    "2. **Sampling Trenches**:\n",
    "   - You can choose between two sampling methods:\n",
    "     - **Sample all trenches at once**: This option allows you to sample all trenches in a single operation.\n",
    "     - **Sample a specific trench**: Alternatively, you can focus on sampling a single trench for more granular analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample by a give arc length edge and resample section\n",
    "# arc_length_edge = 0.0; arc_length_resample_section = 2.0  # by degree\n",
    "arc_length_edge = 2.0; arc_length_resample_section = 2.0  # by degree\n",
    "\n",
    "use_recorded_file = True; resample_all = True; trench_pid = None # use this option to start from a recorded file\n",
    "# use_recorded_file = False; resample_all = True; trench_pid = None\n",
    "# use_recorded_file = False; resample_all = False; trench_pid = 2000\n",
    "\n",
    "# resample the subduction zones\n",
    "recorded_file = os.path.join(ASPECT_LAB_DIR, \"files\", \"ThDSubduction\", \"gplate_json_files\", \"subduction_resampled_t_%.2e.csv\" \\\n",
    "                         % (reconstruction_time))\n",
    "# recorded_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.csv\" \\\n",
    "#                            % (reconstruction_time, arc_length_edge, arc_length_resample_section))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_recorded_file:\n",
    "    print(\"use recorded file: \", recorded_file)\n",
    "    assert(os.path.isfile(recorded_file))\n",
    "    subduction_data_resampled = pd.read_csv(recorded_file) \n",
    "else:\n",
    "    if resample_all:\n",
    "        subduction_data_resampled = ResampleAllSubduction(subduction_data, trench_pids, arc_length_edge, arc_length_resample_section, all_columns)\n",
    "    else:\n",
    "        subduction_data_resampled, _ = ResampleSubductionById(subduction_data, trench_pid, arc_length_edge, arc_length_resample_section, all_columns)\n",
    "    subduction_data_resampled['age'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['lon_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['lat_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['fix_age_polarity'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    # todo_ages\n",
    "    subduction_data_resampled['marker'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['marker_fill'] = ['none' for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['color'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "\n",
    "    # debug\n",
    "    print(len(subduction_data_resampled))\n",
    "\n",
    "    # fix the ages\n",
    "    FixTrenchAge(subduction_data_resampled, age_grid_raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subduction_data_resampled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for invalid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the invalid indexes\n",
    "invalid_indexes = []\n",
    "for i in range(len(subduction_data_resampled['age'])):\n",
    "    if np.isnan(subduction_data_resampled['age'][i]):\n",
    "        invalid_indexes.append(i)\n",
    "\n",
    "print(\"len(ages): \")\n",
    "print(len(subduction_data_resampled['age']))\n",
    "print(\"ages: \")\n",
    "print(subduction_data_resampled['age'])\n",
    "print(\"invalid_indexes: \")\n",
    "print(invalid_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix the invalid values\n",
    "\n",
    "First, map out the invalid ages from the outputs from the previous section.\n",
    "\n",
    "Use the index, theta and direction to create a new sampling point interact with the raster data\n",
    "* i_fs: the index\n",
    "* theta_fs: theta of the direction. 0 is north and 180 is south.\n",
    "* d_fs: distance along the direction\n",
    "\n",
    "The next section is for plotting the current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_fs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "# theta_fs = [180.0, 180.0, 180.0, 180.0, 180.0, 180.0, 180.0, 210.0, 210.0, 150.0, 180.0, 180.0]\n",
    "# d_fs = [100e3, 100e3, 100e3, 100e3, 100e3, 200e3, 400e3, 300e3, 200e3, 300e3, 200e3, 1000e3]\n",
    "# for ii in range(len(i_fs)):\n",
    "#     i_f = i_fs[ii]\n",
    "#     theta_f = theta_fs[ii]\n",
    "#     d_f = d_fs[ii]\n",
    "#     subduction_data_resampled_local = None\n",
    "#     subduction_data_resampled_local = pd.DataFrame([subduction_data_resampled.iloc[i_f]])\n",
    "#     subduction_data_resampled_local.lon, subduction_data_resampled_local.lat = \\\n",
    "#         Utilities.map_point_by_distance(subduction_data_resampled.iloc[i_f].lon, subduction_data_resampled.iloc[i_f].lat, theta_f, d_f)\n",
    "#     new_age = GClass.InterpolateAgeGrid(subduction_data_resampled_local)\n",
    "#     print(i_f, \": new age - \", new_age)\n",
    "#     subduction_data_resampled.loc[i_f, 'age'] = new_age\n",
    "#     if new_age is not np.nan:\n",
    "#         new_lon = subduction_data_resampled_local.lon.values[0]\n",
    "#         new_lat = subduction_data_resampled_local.lat.values[0]\n",
    "#         subduction_data_resampled.loc[i_f, 'lon_fix'] = new_lon\n",
    "#         subduction_data_resampled.loc[i_f, 'lat_fix'] = new_lat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix the dataset of Ryuku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ryuku_dataset = pd.DataFrame(np.nan, index=range(5), columns=subduction_data_resampled.columns)\n",
    "\n",
    "# ryuku_dataset['lat'] = [23.4, 24.2, 25.7, 27.5, 29.8]\n",
    "# ryuku_dataset['lon'] = [124.0, 127.0, 129.0, 130.5, 132.0]\n",
    "# ryuku_dataset['age'] = [35.0, 38.0, 48.0, 50.0, 50.0]\n",
    "# ryuku_dataset['trench_velocity']= [3.0, 0.9, 1.2, 0.7, 0.9]\n",
    "\n",
    "# subduction_data_resampled = pd.concat([subduction_data_resampled, ryuku_dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add a vector to determined the direction of convergence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subduction_data_resampled['conv_angle_polarity'] = [0.0 for i in range(len(subduction_data_resampled))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic plots\n",
    "# plot the reconstructed zone\n",
    "i_p = None # plot all\n",
    "# i_p = 76 # plot one point\n",
    "fig = plt.figure(figsize=(10,6), dpi=100)\n",
    "ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude = 180))\n",
    "\n",
    "# Plot global features such as coastlines and the age grid using the custom plotting function\n",
    "ax = plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "# debug\n",
    "print(subduction_data_resampled['age'])\n",
    "\n",
    "# plot all the fixed ages\n",
    "mask = (~subduction_data_resampled['age'].isna())\n",
    "if i_p is None:\n",
    "    # plot all points\n",
    "    ax.scatter(subduction_data_resampled[mask].lon, subduction_data_resampled[mask].lat, marker=\".\", s=60, c='r', transform=ccrs.PlateCarree())\n",
    "    ax.scatter(subduction_data_resampled[~mask].lon, subduction_data_resampled[~mask].lat, marker=\".\", s=60, c='y', transform=ccrs.PlateCarree())\n",
    "    ax.scatter(subduction_data_resampled[mask].lon_fix, subduction_data_resampled[mask].lat_fix, marker=\".\", s=30, c='c', transform=ccrs.PlateCarree())\n",
    "else:\n",
    "    # plot one point, don't apply mask\n",
    "    ax.scatter(subduction_data_resampled.lon[i_p], subduction_data_resampled.lat[i_p], marker=\".\", s=60, c='r', transform=ccrs.PlateCarree())\n",
    "    ax.scatter(subduction_data_resampled.lon_fix[i_p], subduction_data_resampled.lat_fix[i_p], marker=\".\", s=30, c='c', transform=ccrs.PlateCarree())\n",
    "\n",
    "# write outputs\n",
    "fileout = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.pdf\" \\\n",
    "                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "fileout1 = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.png\" \\\n",
    "                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "if not (os.path.isdir(os.path.dirname(fileout))):\n",
    "    os.mkdir(os.path.dirname(fileout))\n",
    "fig.savefig(fileout)\n",
    "fig.savefig(fileout1)\n",
    "print(\"Save figure: %s\" % fileout)\n",
    "print(\"Save figure: %s\" % fileout1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_file = True\n",
    "# export the file to a temp file\n",
    "if resample_all:\n",
    "    temp_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.csv\" \\\n",
    "                            % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "else:\n",
    "    temp_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_pid_%d_edge_%.2f_section_%.2f.csv\" \\\n",
    "                         % (reconstruction_time, int(trench_pid), arc_length_edge, arc_length_resample_section))\n",
    "\n",
    "# don't mess up the existing files\n",
    "if record_file:\n",
    "    subduction_data_resampled.to_csv(temp_file)\n",
    "    print(\"Data saved to %s\" % temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze extracted data set\n",
    "\n",
    "We plot the convergence / trench retreat rate of trenches below.\n",
    "The plot is also combined with a sea floor age\n",
    "\n",
    "Issue: We get a lot of nan value in ages\n",
    "1. check the position of the sample points\n",
    "2. plot the raster of oceanic plate age\n",
    "\n",
    "#### Some notes\n",
    "\n",
    "* ANDA-SUM: The change from trench retreat to advance (as in L05) is missing\n",
    "* JAVA: essientially, I am fixing the ages to one point (on the corner, above Australia); The trench velocity is retreat, instead of advance (L05)\n",
    "* LUZ: gplately data suggest advance of 2.5, while L05 shows fast retreat motion\n",
    "* Ryuku: data is missing in the current dataset, fixing using values form Table 1 in Lallemend et al., 2005\n",
    "* KuKam: data shows retreat motion, while L05 shows advance motion\n",
    "* PER-NCHI-JUAN-SCHI (subduction id 911): These seem to be all in the subduction id 901. Beyond a trench id of 201, the trench id varies from 201010 to 20101?. Up north, the component is marked with trench id 2031 (Caribbean/farallon subduction iniating at 85 Ma).\n",
    "* South to SCHI: This is represented by the subduction id 801, but the trench id 201 continues from the previous 901 subduction.\n",
    "* ANT: retreat motion in this dataset, rather than the advance motion in L05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate a distance to an adjacent subduction zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Geod\n",
    "\n",
    "Ro = 6371e3\n",
    "    \n",
    "# Initialize the Geod object with the WGS84 ellipsoid model, \n",
    "# which provides accurate geodesic calculations on Earth's surface\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2, radius=Ro):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Distance in the given radius unit (default is kilometers for Earth)\n",
    "    distance = radius * c\n",
    "    return distance\n",
    "\n",
    "# Record both the minimum distance and the indexing in the dataset\n",
    "subduction_distances = np.zeros(len(subduction_data_resampled))\n",
    "subduction_indexes = np.array(range(len(subduction_data_resampled)))\n",
    "subduction_min_distance_indexes = np.full(len(subduction_data_resampled), -1, dtype=int)\n",
    "subduction_maker_lons = []\n",
    "subduction_maker_lats = []\n",
    "\n",
    "# By default, we assign the length of the equator and compare the computed\n",
    "# distance to the previous value\n",
    "# i_p = None; subducting_pid = None\n",
    "# i_p = 142; subducting_pid = None # plot one point\n",
    "# i_p = None; subducting_pid = 201; trench_pid = None # plot one point\n",
    "# i_p = None; subducting_pid = 901; trench_pid = 699 # plot one point\n",
    "\n",
    "# Uncomment this part to modify the values of conv_angle_polarity\n",
    "# if i_p is not None:\n",
    "#   subduction_data_resampled.loc[i_p, \"conv_angle_polarity\"] = 0.0\n",
    "# elif subducting_pid is not None:\n",
    "#   # Figure out the indexes of the selected points and their matching\n",
    "#   # points in the dataset\n",
    "#   mask1 = subduction_data_resampled.subducting_pid == subducting_pid\n",
    "#   if trench_pid is not None:\n",
    "#     mask2 = subduction_data_resampled.trench_pid == trench_pid\n",
    "#   else:\n",
    "#     mask2 = np.ones(len(subduction_data_resampled), dtype=bool)\n",
    "#   mask = (mask1 & mask2)\n",
    "#   subduction_data_resampled.loc[mask, \"conv_angle_polarity\"] = 0.0\n",
    "\n",
    "d_m = 1000e3 # distance of marker point\n",
    "theta_diff = 10.0 # degree\n",
    "for i_1 in range(len(subduction_data_resampled)):\n",
    "  subducting_pid_1 = int(subduction_data_resampled.subducting_pid[i_1])\n",
    "  lat1, lon1 = subduction_data_resampled.lat[i_1], subduction_data_resampled.lon[i_1]\n",
    "  # Pin a marker point with a distance d_m and an angle of convergence (theta, from east)\n",
    "  # Then, get a shortest path connecting the query point and the marker point.\n",
    "  # The direction of convergence varies by assigning a polarity variable\n",
    "  if subduction_data_resampled.conv_angle_polarity[i_1] == 0.0:\n",
    "    theta = subduction_data_resampled.conv_angle[i_1] + 90.0\n",
    "  elif subduction_data_resampled.conv_angle_polarity[i_1] == 1.0:\n",
    "    theta = subduction_data_resampled.conv_angle[i_1] - 90.0\n",
    "  theta = (theta + 360) % 360 # normalize theta\n",
    "  lon_m, lat_m = Utilities.map_point_by_distance(lon1, lat1, theta, d_m)\n",
    "  path_lon_m , path_lat_m = Utilities.shortest_path_between_two_points([lon1, lat1], [lon_m, lat_m], geod.npts, 100)\n",
    "  subduction_maker_lons.append(path_lon_m); subduction_maker_lats.append(path_lat_m) \n",
    "  distance = 2 * np.pi * Ro\n",
    "  for j_2 in range(len(subduction_data_resampled)):\n",
    "    subducting_pid_2 = int(subduction_data_resampled.subducting_pid[j_2])\n",
    "    lat2, lon2 = subduction_data_resampled.lat[j_2], subduction_data_resampled.lon[j_2]\n",
    "    theta_1 = Utilities.calculate_bearing(lon1, lat1, lon2, lat2) # theta_1 is normalized\n",
    "    if j_2 == i_1:\n",
    "      continue\n",
    "    if subducting_pid_1 == subducting_pid_2:\n",
    "      continue\n",
    "    distance_between_points = haversine(lat1, lon1, lat2, lon2)\n",
    "    if distance_between_points < distance and abs(theta_1 - theta) < theta_diff:\n",
    "      distance = distance_between_points\n",
    "      subduction_min_distance_indexes[i_1] = j_2\n",
    "  subduction_distances[i_1] = distance\n",
    "\n",
    "subduction_data_resampled[\"near_distance\"] = subduction_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the matching points of minimum distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_p = None; subducting_pid = None; trench_pid=None # plot one subduction zone\n",
    "# i_p = 43; subducting_pid = None; trench_pid=None # plot one point\n",
    "# i_p = None; subducting_pid = 903; trench_pid=None # plot one subduction zone\n",
    "\n",
    "# First, the global coastline is plotted\n",
    "fig = plt.figure(figsize=(10,6), dpi=100)\n",
    "ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude = 180))\n",
    "plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "mask1 = MaskBySubductionTrenchIds(subduction_data_resampled, subducting_pid, trench_pid, i_p)\n",
    "mask2 = (subduction_min_distance_indexes >= 0)\n",
    "\n",
    "mask = mask1 & mask2  # valid points with trench distance\n",
    "mask_in = mask1 & (~mask2)  # invalid points without trench distance\n",
    "\n",
    "min_indexes_invalid = subduction_min_distance_indexes[mask_in]\n",
    "min_indexes_valid = subduction_min_distance_indexes[mask]\n",
    "indexes_valid = subduction_indexes[mask]\n",
    "\n",
    "\n",
    "# Here we plot the pairs of matching points in the subduction zones\n",
    "# We also plot the marker path along the direction of local convergence\n",
    "# and a length of d_m\n",
    "# plot the query points\n",
    "# debug\n",
    "ax.scatter(subduction_data_resampled.lon[mask], subduction_data_resampled.lat[mask], marker=\".\", s=60, c='r', transform=ccrs.PlateCarree())\n",
    "ax.scatter(subduction_data_resampled.lon[mask_in], subduction_data_resampled.lat[mask_in], marker=\".\", s=60, c='purple', transform=ccrs.PlateCarree())\n",
    "for i in range(len(min_indexes_valid)):\n",
    "  # plot the marker path\n",
    "  ax.scatter(subduction_maker_lons[indexes_valid[i]], subduction_maker_lats[indexes_valid[i]], marker='.', s=5, c='orange', transform=ccrs.PlateCarree())\n",
    "  # plot the matching points\n",
    "  try:\n",
    "    ax.scatter(subduction_data_resampled.lon[min_indexes_valid[i]], subduction_data_resampled.lat[min_indexes_valid[i]], marker=\".\", s=60, c='b', transform=ccrs.PlateCarree())\n",
    "    ax.plot([subduction_data_resampled.lon[indexes_valid[i]], subduction_data_resampled.lon[min_indexes_valid[i]]],\\\n",
    "           [subduction_data_resampled.lat[indexes_valid[i]], subduction_data_resampled.lat[min_indexes_valid[i]]],\\\n",
    "            c='c', transform=ccrs.PlateCarree())\n",
    "  except KeyError:\n",
    "    pass # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subduction_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot results of analysis\n",
    "Here we generate a variation of the previous plot, by differentiating valid age values and invalid age values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary keys represent subduction zone IDs, and the values specify\n",
    "\n",
    "from matplotlib.path import Path\n",
    "# the marker style, face color, and name associated with that ID.\n",
    "# this definition of snowflake initially has an error in the \"code\" part\n",
    "verts = [\n",
    "    (0., 0.),   # Center\n",
    "    (0.2, 0.6), # Upper arm\n",
    "    (0., 0.),   # Center\n",
    "    (0.4, 0.4), # Right diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (0.6, 0.2), # Right arm\n",
    "    (0., 0.),   # Center\n",
    "    (0.4, -0.4),# Right down diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (0.2, -0.6),# Bottom arm\n",
    "    (0., 0.),   # Center\n",
    "    (-0.4, -0.4),# Left down diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (-0.6, -0.2),# Left arm\n",
    "    (0., 0.),   # Center\n",
    "    (-0.4, 0.4),# Left diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (-0.2, 0.6),# Upper left arm\n",
    "]\n",
    "codes = [Path.MOVETO] + [Path.LINETO, Path.MOVETO] * 8 + [Path.MOVETO]\n",
    "snowflake = Path(verts, codes)\n",
    "\n",
    "# Define vertices for two equilateral triangles\n",
    "vertices = [\n",
    "    [0, 1], [-np.sqrt(3)/2, -0.5], [np.sqrt(3)/2, -0.5], [0, 1],  # First triangle\n",
    "    [0, -1], [-np.sqrt(3)/2, 0.5], [np.sqrt(3)/2, 0.5], [0, -1]   # Second triangle\n",
    "]\n",
    "# Flatten the vertices list for creating the Path\n",
    "vertices = np.array(vertices)\n",
    "# Define path codes (all 'LINETO' except the start 'MOVETO')\n",
    "codes = [Path.MOVETO] + [Path.LINETO] * (len(vertices) - 1)\n",
    "\n",
    "star_path = Path(vertices, codes)\n",
    "\n",
    "plot_options = \\\n",
    "{\n",
    "    903: {\"marker\": 'o',  \"markerfacecolor\": \"yellow\", \"name\": \"CAS\"},\n",
    "    511: {\"marker\": 's',  \"markerfacecolor\": \"yellow\", \"name\": \"ANDA-SUM\"},\n",
    "    801: {\"marker\": 'd',  \"markerfacecolor\": \"yellow\", \"name\": \"JAVA\"},\n",
    "    645: {\"marker\": snowflake,  \"markerfacecolor\": \"black\", \"name\": \"SULA\"},\n",
    "    602: {\"marker\": 'x',  \"markerfacecolor\": \"blue\", \"name\": \"LUZ\"},\n",
    "    608: {\"marker\": 's',  \"markerfacecolor\": 'c', \"name\": \"PHIL\"},\n",
    "    901: {\n",
    "        699: {\"marker\": '>',  \"markerfacecolor\": 'red', \"name\": \"MAR\"},\n",
    "        659: {\"marker\": 's',  \"markerfacecolor\": 'red', \"name\": \"IZU\"},\n",
    "        (601112.0, 601118.0): {\"marker\": '^',  \"markerfacecolor\": 'green', \"name\": \"JAP\"},\n",
    "        406:{\"marker\": 'v',  \"markerfacecolor\": 'green', \"name\": \"KUKAM\"},\n",
    "        111: {\"marker\": 'o',  \"markerfacecolor\": 'pink', \"name\": \"ALE-ALA\"},\n",
    "        (806, 821): {\"marker\": 'd',  \"markerfacecolor\": 'blue', \"name\": \"TON-KERM\"}\n",
    "        },\n",
    "    909: {\"marker\": star_path,  \"markerfacecolor\": 'c', \"name\": \"MEX\"},\n",
    "    911: {\"marker\": 'o',  \"markerfacecolor\": 'k', \"name\": \"PER-NCHI-JUAN-SCHI\"},\n",
    "    802: {\"marker\": 'd',  \"markerfacecolor\": 'k', \"name\": \"SSCHI-TBD\"},\n",
    "    201: {\n",
    "        2011:{\"marker\": '+',  \"markerfacecolor\": 'pink', \"name\": \"ANT\"},\n",
    "        815:{\"marker\": '*',  \"markerfacecolor\": 'r', \"name\": \"SAND\"}\n",
    "        },\n",
    "    1: {\"marker\": 'd',  \"markerfacecolor\": \"r\", \"name\": \"RYU\"}\n",
    "}\n",
    "\n",
    "# Create a figure and two subplots for plotting trench velocity data.\n",
    "# `gs` defines a 2x1 grid layout for the subplots.\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "gs = gridspec.GridSpec(3, 1)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax3 = fig.add_subplot(gs[2, 0])\n",
    "total_points_plotted = 0  # Variable to record the total number of plotted points.\n",
    "\n",
    "# Filter out rows with NaN values in the \"age\" column.\n",
    "mask_age = (~subduction_data_resampled[\"age\"].isna())\n",
    "total_points_plotted += len(subduction_data_resampled[mask_age])  # Count valid points.\n",
    "subduction_data_resampled_valid = subduction_data_resampled[mask_age]  # Store valid data.\n",
    "\n",
    "# Obtain a sorted list of unique subducting plate IDs from the valid data.\n",
    "unique_subducting_pids = subduction_data_resampled_valid.subducting_pid.unique()\n",
    "labels = []\n",
    "patches = []\n",
    "unique_subducting_pids.sort()  # Sort the unique subducting plate IDs.\n",
    "print(unique_subducting_pids)\n",
    "\n",
    "# Lookup and store subducting plate names based on their IDs.\n",
    "unique_subducting_names = []\n",
    "for i in range(len(unique_subducting_pids)):\n",
    "    subducting_pid = unique_subducting_pids[i]\n",
    "    # unique_subducting_names.append(GParseReconstruction.LookupNameByPid(int(subducting_id)))\n",
    "    unique_subducting_names.append(LookupNameByPid(trench_pids, name_lookups[\"trench_names\"], int(subducting_pid)))\n",
    "\n",
    "# Loop through each subducting plate ID and plot the corresponding trench velocity.\n",
    "for i in range(len(unique_subducting_pids)):\n",
    "    _name = unique_subducting_names[i]\n",
    "\n",
    "    subducting_id = unique_subducting_pids[i]\n",
    "    try:\n",
    "        plot_option_sub_dict = plot_options[int(subducting_id)]  # Get plot options for the ID.\n",
    "    except KeyError:\n",
    "        # If no specific plot option is found, use default settings.\n",
    "        print(\"Id %s not found, marked as TBD\" % int(subducting_id))\n",
    "        plot_option_sub_dict = {\"marker\": 'o',  \"markerfacecolor\": None, \"name\": \"TBD\"}\n",
    "\n",
    "    # Make an output for the plotting function to loop over the trench ids\n",
    "    plot_trench_pids = None; plot_option_list = None\n",
    "    if 'name' in plot_option_sub_dict:\n",
    "        # A subduction contains a single trench\n",
    "        plot_trench_pids = [None]\n",
    "        plot_option_list = [plot_option_sub_dict.copy()]\n",
    "    else:\n",
    "        # A subduction contains multiple trenches\n",
    "        plot_trench_pids = []\n",
    "        plot_option_list = []\n",
    "        for key, value in plot_option_sub_dict.items():\n",
    "            plot_trench_pids.append(key)\n",
    "            plot_option_list.append(value.copy())\n",
    "\n",
    "    # Loop over the trench ids and plot the markers\n",
    "    for i_tr in range(len(plot_trench_pids)):\n",
    "        trench_pid = plot_trench_pids[i_tr]\n",
    "        plot_option = plot_option_list[i_tr]\n",
    "        # We want trench_pid options to be flexible.\n",
    "        # It could be a - a value; b - a range and c - multiple values\n",
    "        # d - None\n",
    "        # Create a mask for the current subducting plate and plot its trench velocity.\n",
    "        # We allow a variation of 0.1 from the integar value\n",
    "        # mask1 - match the subducting id\n",
    "        # mask2 - match the trench pid condition.\n",
    "        mask1 = (abs(subduction_data_resampled.subducting_pid - subducting_id) < 0.1)\n",
    "        if trench_pid is None:\n",
    "            mask = mask1\n",
    "        elif type(trench_pid) == float or type(trench_pid) == int:\n",
    "            mask = mask1 & (abs(subduction_data_resampled.trench_pid - trench_pid) < 0.1)\n",
    "        elif type(trench_pid) == list:\n",
    "            # mutiple values\n",
    "            mask2 = (abs(subduction_data_resampled.trench_pid - trench_pid[0]) < 0.1)\n",
    "            for trench_sub_pid in trench_pid[1:]:\n",
    "                mask2 = mask2 | (abs(subduction_data_resampled.trench_pid - trench_sub_pid) < 0.1)\n",
    "            mask = mask1 & mask2\n",
    "        elif type(trench_pid) == tuple:\n",
    "            # a range\n",
    "            assert(len(trench_pid) == 2)\n",
    "            mask2 = ((subduction_data_resampled.trench_pid >= trench_pid[0]) & (subduction_data_resampled.trench_pid <= trench_pid[1]))\n",
    "            mask = mask1 & mask2\n",
    "        else:\n",
    "            raise ValueError(\"Type of trench pid is wrong. Possible types are [None, float, int, list, dict]\")\n",
    "        ages = subduction_data_resampled_valid.age[mask]\n",
    "        trench_velocities = subduction_data_resampled_valid.trench_velocity[mask]\n",
    "        near_distances = subduction_data_resampled_valid.near_distance[mask]\n",
    "        _patch = ax1.plot(ages, trench_velocities,\\\n",
    "                marker=plot_option[\"marker\"], markerfacecolor=plot_option[\"markerfacecolor\"],\\\n",
    "                markeredgecolor='black', markersize=10, linestyle='', label=plot_option[\"name\"])[0]\n",
    "        _patch_d = ax3.plot(near_distances, trench_velocities,\\\n",
    "                marker=plot_option[\"marker\"], markerfacecolor=plot_option[\"markerfacecolor\"],\\\n",
    "                markeredgecolor='black', markersize=10, linestyle='', label=plot_option[\"name\"])[0]\n",
    "        patches.append(_patch)\n",
    "\n",
    "i += 1  # Increment index.\n",
    "\n",
    "# Configure grid and legend for the second subplot.\n",
    "ax1.grid()\n",
    "ax3.grid()\n",
    "ax2.legend(handles=patches, bbox_to_anchor=(0.5, 0.5), loc='center', ncol=2, numpoints=1, frameon=False)\n",
    "\n",
    "# Output the total number of plotted points.\n",
    "print(\"Total plotted points: %d\" % total_points_plotted)\n",
    "\n",
    "# Set axis limits and labels for the first plot (trench velocity vs age).\n",
    "ax1.set_xlim([0, 160.0])\n",
    "ax1.set_ylim([-10.0, 10.0])\n",
    "ax3.set_ylim([-10.0, 10.0])\n",
    "ax1.set_xlabel(\"Age (Ma)\")\n",
    "ax1.set_ylabel(\"Trench Velocity Magnitude (cm/yr)\")\n",
    "ax3.set_ylabel(\"Trench Velocity Magnitude (cm/yr)\")\n",
    "\n",
    "# Save the figure to a PDF file with a name derived from the reconstruction parameters.\n",
    "fileout = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_distribution_t_%.2e_edge_%.2f_section_%.2f.pdf\"\\\n",
    "     % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "fig.savefig(fileout)\n",
    "print(\"figure saved: %s\" % fileout)\n",
    "\n",
    "# Save the subducting plate ID and names to a CSV file for future reference.\n",
    "csv_out = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_distribution_t_%.2e_edge_%.2f_section_%.2f.csv\"\\\n",
    "     % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "unique_data = {\n",
    "    \"pid\": unique_subducting_pids,\n",
    "    'name': unique_subducting_names\n",
    "}\n",
    "df_unique_data = pd.DataFrame(unique_data)\n",
    "df_unique_data.to_csv(csv_out)\n",
    "print(\"csv file saved: %s\" % csv_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-gplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
