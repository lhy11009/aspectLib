{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the environment of py-gplate\n",
    "import sys\n",
    "import gplately\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import cartopy.crs as ccrs\n",
    "from plate_model_manager import PlateModelManager\n",
    "import math\n",
    "import re\n",
    "\n",
    "# directory to the aspect Lab\n",
    "# set the location of file outputs\n",
    "ASPECT_LAB_DIR = os.environ['ASPECT_LAB_DIR']\n",
    "base_dir = ASPECT_LAB_DIR\n",
    "RESULT_DIR = os.path.join(base_dir, 'results')\n",
    "Utilities_dir = os.path.join(base_dir, 'utilities', \"python_scripts\")\n",
    "if not os.path.isdir(Utilities_dir):\n",
    "    raise FileNotFoundError(\"The utilities Doesn't exist yet\")\n",
    "# import shilofue.GPlateLib as GPlateLib\n",
    "# import utilities in subdirectiory\n",
    "sys.path.append(Utilities_dir)\n",
    "import Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow for Subduction Dataset from Plate Reconstruction\n",
    "\n",
    "#### Pre-requisites\n",
    "\n",
    "- Download the utilities file at (TODO: package this file)\n",
    "\n",
    "    https://github.com/lhy11009/Utilities/blob/master/python_scripts/Utilities.py\n",
    "\n",
    "and put it into a directory called\n",
    "\n",
    "    \"utilities\"\n",
    "\n",
    "- Install the gplately package (TODO: include a conda file)\n",
    "\n",
    "- One must first use gplate and export a \"reconstruction\" file for the specific timestep. This file serves to look up names and ids of the subduction zones.\n",
    "\n",
    "#### General workflow\n",
    "\n",
    "This workflow defines the steps to query a subduction plate dataset based on plate reconstruction data.\n",
    "\n",
    "To get a good dataset for a time step:\n",
    "\n",
    "1. **Select the Automatic Workflow and get a global dataset**:\n",
    "   - Use the global dataset with the automatic workflow to initiate the data processing. This is a good reference dataset to refine points of respective trenches.\n",
    "\n",
    "\n",
    "2. **Look in the global dataset for a local subset or extract a Local dataset directly**:\n",
    "    - To directly extract a local dataset, look for a id for a trench. Put that in the workflow to extract a single trench.\n",
    "    - When we are satisfied, damp the data in to a finalized dataset.\n",
    "\n",
    "3. **Resample the subduction zones**\"\n",
    "    - To discretinize the subduction zones with the appropriate interval (e.g. 200 km), an option to rample them is included in the notebook.\n",
    "\n",
    "3. **Analyze the Finalzed Dataset** \n",
    "\n",
    "\n",
    "#### Some Notes\n",
    "\n",
    "##### Fixing Subducting Plate Age\n",
    "To accurately determine the age of the subducting plate, we must ensure that the points are correctly pinned to the subducting plate. This involves two workflows:\n",
    "\n",
    "**Automatic Workflow**: By calling the fill_NaNs value in the age raster. This will automatic fill-in the invalid vlaues\n",
    "\n",
    "    age_grid_raster.fill_NaNs(inplace=True)\n",
    "\n",
    "##### Trench pid and subduction pid\n",
    "\n",
    "The **trench PID** and **subduction PID** correspond to the IDs used in the plate reconstruction process (TODO: look carefully in their paper).\n",
    "\n",
    "- **Subduction PID**: The process ID (PID) for the subduction zone, as used in the reconstruction.\n",
    "- **Trench PID**: The process ID (PID) for the trench, as used in the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions\n",
    "\n",
    "- **mask_by_pids**: Generates a mask based on proximity to specified subducting and trench process IDs.\n",
    "- **haversine**: Calculates the great-circle distance between two geographical points on Earth using the Haversine formula.\n",
    "- **ReadFile**: Reads a file of plate reconstruction data to extract subduction zone details, including locations and IDs.\n",
    "- **LookupNameByPid**: Finds the name of a trench given its corresponding plate ID.\n",
    "- **get_one_subduction_by_trench_id**: Retrieves data for a specific subduction zone from a global dataset based on trench ID.\n",
    "- **plot_global_basics**: Plots basic global geological features on a map, including coastlines and geological age grids.\n",
    "- **resample_subduction**: Resamples subduction zone data along its arc length at specified intervals for simplified analysis.\n",
    "- **ResampleAllSubduction**: Resamples all subduction zones in a dataset using specified trench plate IDs.\n",
    "- **ResampleSubductionById**: Resamples data for a specific subduction zone based on its trench plate ID.\n",
    "- **FixTrenchAgeLocal**: Interpolates and fixes missing age values in subduction data using nearby points.\n",
    "- **FixTrenchAge**: Fixes invalid trench age values in subduction data using various age interpolation methods.\n",
    "- **MaskBySubductionTrenchIds**: Creates a mask for subduction data based on specified subducting and trench IDs or user-selected indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_by_pids(subduction_data, subducting_pid_p, trench_pid_p=None):\n",
    "    \"\"\"\n",
    "    Generates a mask based on proximity to subducting and trench process IDs.\n",
    "    \n",
    "    Parameters:\n",
    "        subduction_data (object): Object containing subducting_pid and trench_pid attributes.\n",
    "        subducting_pid_p (float): Target value for the subducting process ID to match.\n",
    "        trench_pid_p (float, optional): Target value for the trench process ID to match.\n",
    "        \n",
    "    Returns:\n",
    "        mask (bool array): Boolean array where True values indicate proximity to specified process IDs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create initial mask based on subducting process ID within a tolerance of 0.1.\n",
    "    mask1 = (abs(subduction_data.subducting_pid - subducting_pid_p) < 0.1)\n",
    "    \n",
    "    # If trench process ID is specified, create an additional mask based on its proximity.\n",
    "    if trench_pid_p is not None:\n",
    "        mask2 = (abs(subduction_data.trench_pid - trench_pid_p) < 0.1)\n",
    "    \n",
    "    # Combine masks using logical AND operation.\n",
    "    mask = mask1 & mask2\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2, radius=6371e3):\n",
    "    \"\"\"\n",
    "    Calculates the great-circle distance between two points on the Earth using the Haversine formula.\n",
    "    \n",
    "    Parameters:\n",
    "        lat1 (float): Latitude of the first point in degrees.\n",
    "        lon1 (float): Longitude of the first point in degrees.\n",
    "        lat2 (float): Latitude of the second point in degrees.\n",
    "        lon2 (float): Longitude of the second point in degrees.\n",
    "        radius (float, optional): Radius of the sphere; default is Earth's radius in meters (6371e3).\n",
    "        \n",
    "    Returns:\n",
    "        distance (float): The distance between the two points in the specified radius unit.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert latitude and longitude values from degrees to radians for calculation.\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Apply the Haversine formula to calculate the angular distance between points.\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Calculate the physical distance by scaling the angular distance by the specified radius.\n",
    "    distance = radius * c\n",
    "    return distance\n",
    "\n",
    "\n",
    "def ReadFile(infile):\n",
    "    \"\"\"\n",
    "    Reads a file of plate reconstruction data and extracts subduction zone information.\n",
    "    \n",
    "    Parameters:\n",
    "        infile (str): The file path to the input data file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - 'n_trench' (int): The number of subduction zones found in the file.\n",
    "            - 'trench_data' (list): A list of arrays containing coordinates for each subduction zone.\n",
    "            - 'trench_names' (list): A list of names for each subduction zone.\n",
    "            - 'trench_pids' (list): A list of plate IDs associated with each subduction zone.\n",
    "            - 'trench_begin_times' (list): A list of the beginning times for each subduction zone.\n",
    "            - 'trench_end_times' (list): A list of the end times for each subduction zone.\n",
    "    \n",
    "    Implementation:\n",
    "        - Opens the input file and iterates through each line.\n",
    "        - Uses flags `sbd_begin` and `sbd_end` to track whether the current section is a subduction zone.\n",
    "        - Extracts coordinates and header information, such as trench names, plate IDs, and time intervals.\n",
    "        - Appends the extracted data into respective lists and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "    infile = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \n",
    "                          \"Muller_etal_2019_PlateBoundaries_no_topologies\", \n",
    "                          \"reconstructed_0.00Ma.xy\")\n",
    "    assert(os.path.isfile(infile))\n",
    "\n",
    "    trench_data = []\n",
    "    trench_names = []\n",
    "    trench_pids = []\n",
    "    trench_begin_times = []\n",
    "    trench_end_times = []\n",
    "\n",
    "    i = 0\n",
    "    temp_l = []  # Stores line indices of each subduction zone section\n",
    "    temp_d = []  # Temporarily holds coordinates of the current subduction zone\n",
    "    n_trench = 0  # Counts the number of subduction zones\n",
    "    sbd_begin = False  # Flag indicating the start of a subduction zone section\n",
    "    sbd_end = False  # Flag indicating the end of a subduction zone section\n",
    "    read = True  # Flag for continuing to read the file\n",
    "\n",
    "    with open(infile, 'r') as fin:\n",
    "        line = fin.readline()\n",
    "        i += 1\n",
    "        while line:\n",
    "            read = True  # Default to continue reading each loop\n",
    "            \n",
    "            # Check if the end of a subduction zone section is reached\n",
    "            if sbd_begin and re.match('^>', line):\n",
    "                sbd_end = True\n",
    "\n",
    "            # Handle the different scenarios based on the flags\n",
    "            if sbd_begin and (not sbd_end):\n",
    "                # Reading subduction zone data\n",
    "                temp_data = line.split()\n",
    "                temp_data = [float(x) for x in temp_data]\n",
    "                temp_d.append(temp_data)\n",
    "            elif sbd_begin and sbd_end:\n",
    "                # Reached the end of a section, store the data and reset flags\n",
    "                trench_data.append(temp_d)\n",
    "                sbd_begin = False\n",
    "                sbd_end = False\n",
    "                read = False\n",
    "            elif re.match('^>SubductionZone', line):\n",
    "                # Found the start of a new subduction zone section\n",
    "                temp_l.append(i)\n",
    "                sbd_begin = True\n",
    "                temp_d = []\n",
    "                # Continue reading the headers of the section\n",
    "                while line and re.match('^>', line):\n",
    "                    line = fin.readline()\n",
    "                    i += 1\n",
    "                    if re.match('^> name', line):\n",
    "                        trench_names.append(Utilities.remove_substrings(line, [\"> name \", '\\n']))\n",
    "                    elif re.match('> reconstructionPlateId', line):\n",
    "                        trench_pids.append(int(Utilities.remove_substrings(line, [\"> reconstructionPlateId \", '\\n'])))\n",
    "                    elif re.match('> validTime TimePeriod <begin> TimeInstant <timePosition>', line):\n",
    "                        temp0 = Utilities.remove_substrings(line, [\"> validTime TimePeriod <begin> TimeInstant <timePosition>\", '</timePosition>.*\\n'])\n",
    "                        trench_begin_times.append(float(temp0))\n",
    "                        temp1 = Utilities.remove_substrings(line, ['^.*<end> TimeInstant <timePosition>', '</timePosition>.*\\n'])\n",
    "                        trench_end_times.append(float(temp1) if type(temp1) == float else 0.0)\n",
    "                read = False\n",
    "            \n",
    "            if read:\n",
    "                line = fin.readline()\n",
    "                i += 1\n",
    "\n",
    "    i -= 1  # Adjust for the last unsuccessful read\n",
    "    n_trench = len(trench_data)\n",
    "\n",
    "    outputs = {\n",
    "        \"n_trench\": n_trench, \n",
    "        \"trench_data\": trench_data, \n",
    "        \"trench_names\": trench_names,\n",
    "        \"trench_pids\": trench_pids, \n",
    "        \"trench_begin_times\": trench_begin_times, \n",
    "        \"trench_end_times\": trench_end_times\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def LookupNameByPid(trench_pids, trench_names, pid):\n",
    "    \"\"\"\n",
    "    Looks up the name of a trench using its plate ID.\n",
    "\n",
    "    Parameters:\n",
    "        trench_pids (list): A list of plate IDs corresponding to subduction zones.\n",
    "        trench_names (list): A list of names corresponding to the trench IDs.\n",
    "        pid (int): The plate ID for which the trench name is being looked up.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the trench corresponding to the given plate ID. Returns an empty\n",
    "             string if the plate ID is not found.\n",
    "    \n",
    "    Implementation:\n",
    "        - Asserts that the `pid` provided is of type `int`.\n",
    "        - Attempts to find the index of `pid` in the `trench_pids` list.\n",
    "        - If the `pid` is found, retrieves the name from `trench_names` using the index.\n",
    "        - If the `pid` is not found (raises a `ValueError`), returns an empty string.\n",
    "    \"\"\"\n",
    "    _name = \"\"\n",
    "    assert(type(pid) == int)\n",
    "    try:\n",
    "        _index = trench_pids.index(pid)\n",
    "    except ValueError:\n",
    "        _name = \"\"\n",
    "    else:\n",
    "        _name = trench_names[_index]\n",
    "    return _name\n",
    "\n",
    "\n",
    "def get_one_subduction_by_trench_id(subduction_data, trench_pid, all_columns):\n",
    "    \"\"\"\n",
    "    Extracts data for a specific subduction zone from the global dataset at a given reconstruction time.\n",
    "\n",
    "    Parameters:\n",
    "        subduction_data (list): The global dataset of subduction zones at a particular reconstruction time.\n",
    "                                Each element in this list represents a subduction zone and contains values\n",
    "                                corresponding to columns in `all_columns`.\n",
    "        trench_pid (int): The ID of the specific subduction zone to extract.\n",
    "        all_columns (list): The list of column names for the final DataFrame, containing 10 entries:\n",
    "                            ['lon', 'lat', 'conv_rate', 'conv_angle', 'trench_velocity', \n",
    "                             'trench_velocity_angle', 'arc_length', 'trench_azimuth_angle',\n",
    "                             'subducting_pid', 'trench_pid'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the data for the specified subduction zone, \n",
    "                      sorted by latitude (column index 1).\n",
    "    \n",
    "    Implementation:\n",
    "        - Initializes an empty list `ret` to store the selected subduction data.\n",
    "        - Iterates over `subduction_data` and appends rows that match the `trench_pid` at index 9.\n",
    "        - Sorts the collected rows by latitude (index 1) to order them spatially.\n",
    "        - Converts the sorted list into a pandas DataFrame using `all_columns` as headers.\n",
    "        - Returns the resulting DataFrame.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    Found = False\n",
    "    for row in subduction_data:\n",
    "        if row[9] == trench_pid:  # Only select rows where the trench ID matches\n",
    "            ret.append(row)\n",
    "            Found = True\n",
    "\n",
    "    # Assert the the given pid is contained in the subduction_data\n",
    "    assert(Found)\n",
    "    \n",
    "    # Sort the selected data by latitude (index 1)\n",
    "    ret.sort(key=lambda row: row[1])\n",
    "    \n",
    "    # Create a DataFrame with the selected and sorted data\n",
    "    one_subduction_data = pd.DataFrame(ret, columns=all_columns)\n",
    "    \n",
    "    return one_subduction_data\n",
    "\n",
    "\n",
    "def plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time):\n",
    "    \"\"\"\n",
    "    Plots basic global geological features on a given axis, including coastlines and an age grid.\n",
    "\n",
    "    Parameters:\n",
    "        ax (matplotlib.axes._axes.Axes): The axis on which to plot the global features.\n",
    "        gplot (gplately.plot.PlotTopologies): An object used for plotting geological features such as coastlines.\n",
    "        age_grid_raster: A raster object containing age data, typically used for visualizing geological ages.\n",
    "        reconstruction_time (float): The geological time at which the reconstruction is plotted.\n",
    "    \n",
    "    Implementation:\n",
    "        - Configures global gridlines on the plot with specific color, linestyle, and locations.\n",
    "        - Sets the map extent to global.\n",
    "        - Uses the `gplot` object to plot coastlines at the given reconstruction time.\n",
    "        - Plots the age grid data on the map using a specified colormap and transparency level.\n",
    "        - Adds a color bar to the plot to represent ages, with a labeled color bar axis.\n",
    "    \"\"\"\n",
    "    # Configure global gridlines with specified color and linestyle\n",
    "    gl = ax.gridlines(color='0.7', linestyle='--', xlocs=np.arange(-180, 180, 15), ylocs=np.arange(-90, 90, 15))\n",
    "    gl.left_labels = True\n",
    "\n",
    "    # Set the map extent to global\n",
    "    ax.set_global()\n",
    "\n",
    "    # Set the reconstruction time for the gplot object and plot coastlines in grey\n",
    "    gplot.time = reconstruction_time\n",
    "    gplot.plot_coastlines(ax, color='grey')\n",
    "\n",
    "    # Plot the age grid on the map using a colormap from yellow to blue\n",
    "    im_age = gplot.plot_grid(ax, age_grid_raster.data, cmap='YlGnBu', vmin=0, vmax=200, alpha=0.8)\n",
    "\n",
    "    # Add a color bar for the age grid with a label\n",
    "    cbar_age = plt.colorbar(im_age)\n",
    "    cbar_age.ax.get_yaxis().labelpad = 15\n",
    "    cbar_age.ax.set_ylabel(\"Age (Ma)\", rotation=90)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def resample_subduction(one_subduction_data, arc_length_edge, arc_length_resample_section, all_columns, **kwargs):\n",
    "    \"\"\"\n",
    "    Resamples data points from a dense subduction zone at specified intervals along its arc length.\n",
    "    This helps simplify and extract key properties of the subduction zone for plotting and analysis.\n",
    "\n",
    "    Parameters:\n",
    "        one_subduction_data (pd.DataFrame): A pandas DataFrame containing data for a single subduction zone.\n",
    "        arc_length_edge (float): The arc length distance from the edges where no resampling is performed.\n",
    "        arc_length_resample_section (float): The interval at which the arc length is resampled.\n",
    "        all_columns (list): A list of column names for the output DataFrame.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "            - indent (int, optional): Indentation for the output log content. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - pd.DataFrame: A DataFrame of the resampled subduction zone data.\n",
    "            - str: A log of the resampled points' coordinates for debugging or output purposes.\n",
    "\n",
    "    Implementation:\n",
    "        - Initializes variables, including indentation and a log for output content.\n",
    "        - Computes cumulative arc lengths for all points in the original data.\n",
    "        - Determines resampling points centered at the midpoint of the arc length and propagates outward.\n",
    "        - Resamples properties by linear interpolation between points, including special handling of longitude and latitude.\n",
    "        - Collects and logs each resampled point's coordinates, and returns the resampled DataFrame and the log.\n",
    "    \"\"\"\n",
    "    # Initialize variables, including default indentation for output\n",
    "    indent = kwargs.get(\"indent\", 0)  # Default is no indentation\n",
    "    log_output_contents = \"\"\n",
    "    data_len = len(one_subduction_data)\n",
    "    \n",
    "    # Compute cumulative arc lengths\n",
    "    arc_lengths = one_subduction_data['arc_length']\n",
    "    arc_length_sums = np.zeros(data_len)\n",
    "    arc_length_sums[0] = arc_lengths[0]\n",
    "    for i in range(1, data_len):\n",
    "        arc_length_sums[i] = arc_length_sums[i - 1] + arc_lengths[i]\n",
    "\n",
    "    # Compute resampling points: start at the center and propagate outward\n",
    "    temp = []\n",
    "    if arc_length_sums[-1] > 2 * arc_length_edge:\n",
    "        temp.append(arc_length_sums[-1] / 2.0)\n",
    "    i = 1\n",
    "    arc_length_sum_temp = arc_length_sums[-1] / 2.0 - arc_length_resample_section / 2.0\n",
    "    arc_length_sum_temp1 = arc_length_sums[-1] / 2.0 + arc_length_resample_section / 2.0\n",
    "    while arc_length_sum_temp > arc_length_edge:\n",
    "        temp.append(arc_length_sum_temp)\n",
    "        temp.append(arc_length_sum_temp1)\n",
    "        arc_length_sum_temp -= arc_length_resample_section\n",
    "        arc_length_sum_temp1 += arc_length_resample_section\n",
    "    arc_length_sums_resampled = sorted(temp)\n",
    "\n",
    "    # Resample properties of the subduction zone by interpolation\n",
    "    one_subduction_data_resampled = pd.DataFrame(columns=all_columns)\n",
    "    i_sbd_re = 0\n",
    "    is_first = True\n",
    "    for arc_length_sum_resampled in arc_length_sums_resampled:\n",
    "        for i in range(len(arc_length_sums) - 1):\n",
    "            if (arc_length_sums[i] <= arc_length_sum_resampled) and (arc_length_sum_resampled < arc_length_sums[i + 1]):\n",
    "                # Calculate the interpolation fraction\n",
    "                fraction = (arc_length_sum_resampled - arc_length_sums[i]) / (arc_length_sums[i + 1] - arc_length_sums[i])\n",
    "                row_temp = fraction * one_subduction_data.iloc[i] + (1. - fraction) * one_subduction_data.iloc[i + 1]\n",
    "                \n",
    "                # Interpolate longitude and latitude using a custom mapping method\n",
    "                row_temp.loc[\"lon\"], row_temp.loc[\"lat\"] = Utilities.map_mid_point(\n",
    "                    one_subduction_data.iloc[i].lon, one_subduction_data.iloc[i].lat,\n",
    "                    one_subduction_data.iloc[i + 1].lon, one_subduction_data.iloc[i + 1].lat, fraction\n",
    "                )\n",
    "\n",
    "                # Log the resampled point's coordinates\n",
    "                log_output_contents += \"%s%d th resampled point: (%.2f, %.2f)\\n\" % (\" \" * indent, i_sbd_re, row_temp.lon, row_temp.lat)\n",
    "                \n",
    "                # Append the interpolated row to the resampled DataFrame\n",
    "                if is_first:\n",
    "                    one_subduction_data_resampled = pd.DataFrame([row_temp])\n",
    "                    is_first = False\n",
    "                else:\n",
    "                    one_subduction_data_resampled = pd.concat([one_subduction_data_resampled, pd.DataFrame([row_temp])], ignore_index=True)\n",
    "        i_sbd_re += 1\n",
    "\n",
    "    return one_subduction_data_resampled, log_output_contents\n",
    "\n",
    "\n",
    "def ResampleAllSubduction(subduction_data, trench_pids, arc_length_edge, arc_length_resample_section, all_columns):\n",
    "    \"\"\"\n",
    "    Resamples all specified subduction zones in the dataset by their trench plate IDs.\n",
    "\n",
    "    Parameters:\n",
    "        subduction_data (pd.DataFrame): The global dataset of subduction zones at a reconstruction time.\n",
    "        trench_pids (list): A list of trench plate IDs for which to perform resampling.\n",
    "        arc_length_edge (float): The arc length distance from the edges where no resampling is performed.\n",
    "        arc_length_resample_section (float): The interval at which the arc length is resampled.\n",
    "        all_columns (list): A list of column names for the final resampled DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the resampled data for all specified subduction zones.\n",
    "    \n",
    "    Implementation:\n",
    "        - Initializes an empty DataFrame and a string to collect log output.\n",
    "        - Iterates through each `trench_pid` in `trench_pids`:\n",
    "            - Extracts data for the subduction zone with `trench_pid`.\n",
    "            - Calls `resample_subduction` to resample the data, logging the process.\n",
    "            - Appends the resampled data to the overall DataFrame.\n",
    "        - Concatenates all resampled subduction data into a single DataFrame and returns it.\n",
    "    \"\"\"\n",
    "    subduction_data_resampled = None\n",
    "    log_output_contents = \"\"\n",
    "\n",
    "    # Iterate over each trench plate ID and resample the subduction zone data\n",
    "    for i in range(len(trench_pids)):\n",
    "        trench_pid = trench_pids[i]\n",
    "        \n",
    "        # Extract data for the current subduction zone\n",
    "        one_subduction_data = get_one_subduction_by_trench_id(subduction_data, trench_pid, all_columns)\n",
    "        \n",
    "        # Resample the subduction data and collect log output\n",
    "        one_subduction_data_resampled, log_output_contents = resample_subduction(\n",
    "            one_subduction_data, arc_length_edge, arc_length_resample_section, all_columns, indent=4\n",
    "        )\n",
    "\n",
    "        # Add information about the start and end points of the subduction zone\n",
    "        log_output_contents += \"%d th arc\\n\" % i\n",
    "        log_output_contents += \"start (%.2f, %.2f)\\n\" % (one_subduction_data.iloc[0].lon, one_subduction_data.iloc[0].lat)\n",
    "        log_output_contents += \"end (%.2f, %.2f)\\n\" % (one_subduction_data.iloc[-1].lon, one_subduction_data.iloc[-1].lat)\n",
    "\n",
    "        # Initialize or concatenate the resampled data into the main DataFrame\n",
    "        if i == 0:\n",
    "            subduction_data_resampled = one_subduction_data_resampled\n",
    "        else:\n",
    "            if not one_subduction_data_resampled.empty:\n",
    "                subduction_data_resampled = pd.concat(\n",
    "                    [subduction_data_resampled, one_subduction_data_resampled], ignore_index=True\n",
    "                )\n",
    "\n",
    "    subduction_data_resampled = pd.DataFrame(subduction_data_resampled, columns=all_columns)\n",
    "\n",
    "    return subduction_data_resampled\n",
    "\n",
    "\n",
    "def ResampleSubductionById(subduction_data, trench_pid, arc_length_edge, arc_length_resample_section, all_columns):\n",
    "    \"\"\"\n",
    "    Resamples a specific subduction zone from the global dataset using its trench plate ID.\n",
    "\n",
    "    Parameters:\n",
    "        subduction_data (pd.DataFrame): The global dataset of subduction zones at a reconstruction time.\n",
    "        trench_pid (int): The ID of the trench subduction zone to resample.\n",
    "        arc_length_edge (float): The arc length distance from the edges where no resampling is performed.\n",
    "        arc_length_resample_section (float): The interval at which the arc length is resampled.\n",
    "        all_columns (list): A list of column names for the final resampled DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: A DataFrame of the resampled subduction zone data.\n",
    "            - str: A log of the resampled points' details for debugging or output purposes.\n",
    "    \n",
    "    Implementation:\n",
    "        - Extracts data for the subduction zone using `get_one_subduction_by_trench_id`.\n",
    "        - Calls `resample_subduction` to resample the data based on the provided parameters.\n",
    "        - Returns the resampled DataFrame and the log output string.\n",
    "    \"\"\"\n",
    "    # Extract data for the specified subduction zone using the trench plate ID\n",
    "    one_subduction_data = get_one_subduction_by_trench_id(subduction_data, trench_pid)\n",
    "\n",
    "    # Resample the subduction zone data and get the log output\n",
    "    one_subduction_data_resampled, log_output_contents = resample_subduction(\n",
    "        one_subduction_data, arc_length_edge, arc_length_resample_section, all_columns, indent=4\n",
    "    )\n",
    "\n",
    "    return one_subduction_data_resampled, log_output_contents\n",
    "\n",
    "\n",
    "def FixTrenchAgeLocal(subduction_data, age_grid_raster, i_p, theta):\n",
    "    \"\"\"\n",
    "    Fixes invalid age values in a subduction data object using age interpolation\n",
    "    from nearby points along a specified direction.\n",
    "\n",
    "    Parameters:\n",
    "        subduction_data (pd.DataFrame): The dataset containing subduction zone data.\n",
    "        age_grid_raster: A raster object containing age data, typically used for visualizing geological ages.\n",
    "        i_p (int): The index of the subduction data point to be fixed.\n",
    "        theta (float): The direction (in degrees) to search for new data points for interpolation.\n",
    "\n",
    "    Returns:\n",
    "        float: The newly interpolated age value. If interpolation fails, returns NaN.\n",
    "    \n",
    "    Implementation:\n",
    "        - Defines a set of distances `ds` to search for new points around the specified index.\n",
    "        - Iterates over pairs of distances to generate two nearby points in the specified direction.\n",
    "        - Uses `Utilities.map_point_by_distance` to calculate the longitude and latitude of the new points.\n",
    "        - If both ages are valid, interpolates between them to determine the new age.\n",
    "        - Updates the `subduction_data` object with the interpolated age and records the fixed location.\n",
    "        - If interpolation is not successful, sets the age to NaN.\n",
    "    \"\"\"\n",
    "    ds = [12.5e3, 25e3, 50e3, 75e3, 100e3, 150e3, 200e3, 300e3, 400e3]\n",
    "    new_age = np.nan\n",
    "\n",
    "    # Iterate over the distances to generate two points for age interpolation\n",
    "    for j in range(len(ds) - 1):\n",
    "        # Generate two local points at distances `ds[j]` and `ds[j+1]` in the direction `theta`\n",
    "        subduction_data_local0 = pd.DataFrame([subduction_data.iloc[i_p]])\n",
    "        subduction_data_local1 = pd.DataFrame([subduction_data.iloc[i_p]])\n",
    "        \n",
    "        subduction_data_local0.loc[:, \"lon\"], subduction_data_local0.loc[:, \"lat\"] = Utilities.map_point_by_distance(\n",
    "            subduction_data.iloc[i_p].lon, subduction_data.iloc[i_p].lat, theta, ds[j]\n",
    "        )\n",
    "        subduction_data_local1.loc[:, \"lon\"], subduction_data_local1.loc[:, \"lat\"] = Utilities.map_point_by_distance(\n",
    "            subduction_data.iloc[i_p].lon, subduction_data.iloc[i_p].lat, theta, ds[j + 1]\n",
    "        )\n",
    "        \n",
    "        # Interpolate ages at the two new points\n",
    "        new_age0 = age_grid_raster.interpolate(subduction_data_local0.lon, subduction_data_local0.lat, method=\"nearest\")\n",
    "        new_age1 = age_grid_raster.interpolate(subduction_data_local1.lon, subduction_data_local1.lat, method=\"nearest\")\n",
    "        \n",
    "        # If both ages are valid, perform interpolation and update the subduction data\n",
    "        if (not np.isnan(new_age0)) and (not np.isnan(new_age1)):\n",
    "            new_age = (new_age0 * ds[j + 1] - new_age1 * ds[j]) / (ds[j + 1] - ds[j])\n",
    "            subduction_data.loc[i_p, \"age\"] = new_age\n",
    "            # debug\n",
    "            subduction_data.loc[i_p, \"lon_fix\"] = subduction_data_local1.lon.iloc[0]  # Records the further point\n",
    "            subduction_data.loc[i_p, \"lat_fix\"] = subduction_data_local0.lat.iloc[0]  # Records the closer point\n",
    "            break\n",
    "        else:\n",
    "            subduction_data.loc[i_p, \"age\"] = np.nan  # Mark as NaN if interpolation fails\n",
    "\n",
    "    return new_age\n",
    "\n",
    "\n",
    "def FixTrenchAge(subduction_data, age_grid_raster, **kwargs):\n",
    "    '''\n",
    "    Fix the trench ages in subduction_data\n",
    "    Inputs:\n",
    "        subduction_data: pandas object, subduction dataset\n",
    "        age_grid_raster: A raster object containing age data, typically used for visualizing geological ages.\n",
    "    '''\n",
    "    # automatically fix the invalid ages \n",
    "    for i in range(len(subduction_data)):\n",
    "        fix_age_polarity = subduction_data.fix_age_polarity[i]\n",
    "        if not np.isnan(fix_age_polarity):\n",
    "            # fix with existing polarity\n",
    "            # 0 and 1: on different side of the trench\n",
    "            # 2: manually assign values of longitude and latitude\n",
    "            if (fix_age_polarity == 0): \n",
    "                new_age = FixTrenchAgeLocal(subduction_data, age_grid_raster, i, subduction_data.trench_azimuth_angle[i] + 180.0)\n",
    "            elif (fix_age_polarity == 1): \n",
    "                new_age = FixTrenchAgeLocal(subduction_data, age_grid_raster, i, subduction_data.trench_azimuth_angle[i])\n",
    "            elif (fix_age_polarity == 2):\n",
    "                subduction_data_local0 = pd.DataFrame([subduction_data.iloc[i]])\n",
    "                subduction_data_local0.loc[:, \"lon\"], subduction_data_local0.loc[:, \"lat\"] = subduction_data.iloc[i].lon_fix, subduction_data.iloc[i].lat_fix\n",
    "                new_age = age_grid_raster.interpolate(subduction_data_local0.lon, subduction_data_local0.lat, method=\"nearest\")\n",
    "                subduction_data.loc[i, 'age'] = new_age\n",
    "                pass\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            # figure out a possible polarity\n",
    "            new_age = FixTrenchAgeLocal(subduction_data, age_grid_raster, i, subduction_data.trench_azimuth_angle[i] + 180.0)\n",
    "            if np.isnan(new_age):\n",
    "                # next, try the other direction\n",
    "                new_age = FixTrenchAgeLocal(subduction_data, age_grid_raster, i, subduction_data.trench_azimuth_angle[i])\n",
    "                if not np.isnan(new_age):\n",
    "                    subduction_data.loc[i, \"fix_age_polarity\"] = 1\n",
    "            else:\n",
    "                subduction_data.loc[i, \"fix_age_polarity\"] = 0\n",
    "\n",
    "\n",
    "def MaskBySubductionTrenchIds(subduction_data, subducting_pid, trench_pid, i_p):\n",
    "    \"\"\"\n",
    "    Generates a combined mask for subduction data based on user selection or specific \n",
    "    subducting and trench IDs.\n",
    "    \n",
    "    Parameters:\n",
    "        subduction_data (pd.DataFrame): The DataFrame containing subduction data to be filtered.\n",
    "        subducting_pid (int or None): The subducting plate ID to match. If None, all IDs are included.\n",
    "        trench_pid (int or None): The trench plate ID to match. If None, all IDs are included.\n",
    "        i_p (list or None): List of indices selected by the user. If not None, these indices are used.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A boolean mask combining the specified conditions for filtering the data.\n",
    "    \n",
    "    Implementation:\n",
    "        - If `i_p` is provided, create `mask1` to select only those indices.\n",
    "        - If `subducting_pid` is provided, create `mask1` to select rows matching the `subducting_pid`.\n",
    "        - If neither is provided, `mask1` includes all rows.\n",
    "        - If `trench_pid` is provided, create `mask2` to select rows matching the `trench_pid`.\n",
    "        - If `trench_pid` is not provided, `mask2` includes all rows.\n",
    "        - The final mask is the logical AND of `mask1` and `mask2`.\n",
    "    \"\"\"\n",
    "    if i_p is not None:\n",
    "        mask1 = np.zeros(len(subduction_data), dtype=bool)\n",
    "        mask1[i_p] = 1\n",
    "    elif subducting_pid is not None:\n",
    "        # Generate mask1 based on the provided subducting plate ID\n",
    "        mask1 = subduction_data.subducting_pid == subducting_pid\n",
    "    else:\n",
    "        mask1 = np.ones(len(subduction_data), dtype=bool)\n",
    "\n",
    "    if trench_pid is not None:\n",
    "        # Generate mask2 based on the provided trench plate ID\n",
    "        mask2 = subduction_data.trench_pid == trench_pid\n",
    "    else:\n",
    "        mask2 = np.ones(len(subduction_data), dtype=bool)\n",
    "\n",
    "    return (mask1 & mask2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "At first run, run these tests by setting run_tests = True.\n",
    "Afterward, set this option to False to skip the test\n",
    "\n",
    "Below is a description of the tests\n",
    "\n",
    "- **test_name_lookup_read_file**: Verifies the `ReadFile` function's ability to correctly read and parse subduction zone data by checking the count of zones and plate ID occurrences.\n",
    "- **test_main_workflow**: Tests the main workflow by initializing plate reconstruction components, performing subduction zone tessellation, and verifying the resampled subduction data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests = True\n",
    "\n",
    "def test_name_lookup_read_file():\n",
    "    \"\"\"\n",
    "    Tests the `ReadFile` function to ensure it correctly reads and parses subduction zone data.\n",
    "    \n",
    "    Implementation:\n",
    "        - Loads the subduction name lookup file from a specified path.\n",
    "        - Calls the `ReadFile` function with the provided file path.\n",
    "        - Asserts that the number of subduction zones (`n_trench`) is as expected.\n",
    "        - Asserts that the plate ID 201 appears the expected number of times in the list of trench plate IDs.\n",
    "    \n",
    "    Assertions:\n",
    "        - The test checks if `n_trench` equals 52, confirming the total count of trenches.\n",
    "        - The test ensures that the plate ID 201 appears 7 times, validating the parsing of plate IDs.\n",
    "    \"\"\"\n",
    "    subduction_name_lookup_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \n",
    "                                               \"Muller_etal_2019_PlateBoundaries_no_topologies\", \n",
    "                                               \"reconstructed_0.00Ma.xy\")\n",
    "\n",
    "    outputs = ReadFile(subduction_name_lookup_file)\n",
    "    assert(outputs['n_trench'] == 52)\n",
    "    assert(outputs[\"trench_pids\"].count(201) == 7)\n",
    "\n",
    "def test_main_workflow():\n",
    "\n",
    "    # assign a reconstruction time\n",
    "    reconstruction_time=0 # time of reconstruction, must be integar\n",
    "\n",
    "    # enter the directory of the plate reconstruction files\n",
    "    # dir_re = os.path.join(ASPECT_LAB_DIR, \"dtemp/gplate_export_test0/Muller_etal_2019_PlateBoundaries_no_topologies\")\n",
    "    # subduction_name_lookup_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"Muller_etal_2019_PlateBoundaries_no_topologies\", \"reconstructed_0.00Ma.xy\")\n",
    "\n",
    "    # fact checks\n",
    "    assert(type(reconstruction_time) == int)\n",
    "    # assert(os.path.isdir(dir_re))\n",
    "\n",
    "    # Initiation\n",
    "    # Initialize the anchor plate ID for the reconstruction model\n",
    "    anchor_plate_id = 0\n",
    "\n",
    "    # Define the columns used in the subduction data DataFrame\n",
    "    all_columns = ['lon', 'lat', 'conv_rate', 'conv_angle', 'trench_velocity', \n",
    "                            'trench_velocity_angle', 'arc_length', 'trench_azimuth_angle', \n",
    "                            'subducting_pid', 'trench_pid']\n",
    "\n",
    "    # Create an instance of the PlateModelManager to manage plate models\n",
    "    pm_manager = PlateModelManager()\n",
    "\n",
    "    # Load the \"Muller2019\" plate model from the specified data directory\n",
    "    plate_model = pm_manager.get_model(\"Muller2019\", data_dir=\"plate-model-repo\")\n",
    "\n",
    "    # Set up the PlateReconstruction model using the loaded plate model data\n",
    "    # This includes rotation models, topologies, and static polygons, with the specified anchor plate ID\n",
    "    model = gplately.PlateReconstruction(\n",
    "        plate_model.get_rotation_model(), \n",
    "        plate_model.get_topologies(), \n",
    "        plate_model.get_static_polygons(),\n",
    "        anchor_plate_id=anchor_plate_id\n",
    "    )\n",
    "\n",
    "    # Initialize the plotting object for visualizing topologies\n",
    "    # The layers used for plotting include coastlines, continental polygons, and COBs (Continental Ocean Boundaries)\n",
    "    gplot = gplately.plot.PlotTopologies(\n",
    "        model, \n",
    "        plate_model.get_layer('Coastlines'), \n",
    "        plate_model.get_layer('ContinentalPolygons'), \n",
    "        plate_model.get_layer('COBs')\n",
    "    )\n",
    "\n",
    "    # Initialize the reconstruction time at 0 (current time)\n",
    "    reconstruction_time = 0\n",
    "\n",
    "    # Initialize variables to hold subduction data and trench IDs\n",
    "    subduction_data = None\n",
    "\n",
    "    # Initialize the age grid raster, which will be used for age-related computations\n",
    "    age_grid_raster = None\n",
    "\n",
    "    # get the reconstruction of subduction zones\n",
    "    subduction_data = model.tessellate_subduction_zones(reconstruction_time, \n",
    "                                                    # tessellation_threshold_radians=0.01, \n",
    "                                                        anchor_plate_id=anchor_plate_id,\n",
    "                                                        ignore_warnings=True)\n",
    "    # get all the trench ids\n",
    "    temp = [row[9] for row in subduction_data]\n",
    "    trench_pids = sorted(set(temp))\n",
    "\n",
    "    # get the age grid raster\n",
    "    age_grid_raster = gplately.Raster(\n",
    "                                    data=plate_model.get_raster(\"AgeGrids\",reconstruction_time),\n",
    "                                    plate_reconstruction=model,\n",
    "                                    extent=[-180, 180, -90, 90]\n",
    "                                    )\n",
    "\n",
    "    age_grid_raster.fill_NaNs(inplace=True)\n",
    "\n",
    "    arc_length_edge = 2.0\n",
    "    arc_length_resample_section = 2.0\n",
    "\n",
    "    subduction_data_resampled = ResampleAllSubduction(subduction_data, trench_pids, arc_length_edge, arc_length_resample_section, all_columns)\n",
    "    \n",
    "    subduction_data_resampled.loc[:, 'age'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'lon_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'lat_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'fix_age_polarity'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'marker'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'marker_fill'] = ['none' for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled.loc[:, 'color'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "\n",
    "    assert(\"trench_azimuth_angle\" in subduction_data_resampled.columns and \"arc_length\" in subduction_data_resampled.columns)\n",
    "\n",
    "\n",
    "\n",
    "if run_tests:\n",
    "    test_name_lookup_read_file()\n",
    "    test_main_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Begin by setting a **reconstruction time**, which `gplately` will use to reconstruct geological features at a specific moment in geological history. Additionally, define an **anchor plate ID**â€”this serves as a reference plate, stabilizing the reconstruction to this particular plate (TODO: research further for detailed impact).\n",
    "\n",
    "You'll also need a **subduction name lookup file** that links each subduction zone's name with its corresponding ID for each reconstruction time. Note that this file is not provided by `gplately` and must be exported separately from GPlates. (TODO: consider making this file optional in the workflow to enhance flexibility.)\n",
    "\n",
    "#### Parameters\n",
    "* **reconstruction_time** - The specific geological time for the reconstruction.\n",
    "* **subduction_name_lookup_file** - Exported file from GPlates containing subduction zone names and their IDs, as `gplately` does not retain this information.\n",
    "* **anchor_plate_id** - The plate used as a reference for stabilizing the reconstruction.\n",
    "* **all_columns** - A list of column names used in the GPlates data to ensure consistent data mapping.\n",
    "* **plate_model** - Specifies the reconstruction model to use; refer to the `gplately` tutorial for additional guidance on available models and usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a reconstruction time\n",
    "reconstruction_time=0 # time of reconstruction, must be integar\n",
    "\n",
    "# enter the directory of the plate reconstruction files\n",
    "dir_re = os.path.join(ASPECT_LAB_DIR, \"dtemp/gplate_export_test0/Muller_etal_2019_PlateBoundaries_no_topologies\")\n",
    "subduction_name_lookup_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"Muller_etal_2019_PlateBoundaries_no_topologies\", \"reconstructed_0.00Ma.xy\")\n",
    "\n",
    "# fact checks\n",
    "assert(type(reconstruction_time) == int)\n",
    "assert(os.path.isdir(dir_re))\n",
    "\n",
    "# Initialize the anchor plate ID for the reconstruction model\n",
    "anchor_plate_id = 0\n",
    "\n",
    "# Define the columns used in the subduction data DataFrame\n",
    "all_columns = ['lon', 'lat', 'conv_rate', 'conv_angle', 'trench_velocity', \n",
    "                          'trench_velocity_angle', 'arc_length', 'trench_azimuth_angle', \n",
    "                          'subducting_pid', 'trench_pid']\n",
    "\n",
    "# Create an instance of the PlateModelManager to manage plate models\n",
    "pm_manager = PlateModelManager()\n",
    "\n",
    "# Load the \"Muller2019\" plate model from the specified data directory\n",
    "plate_model = pm_manager.get_model(\"Muller2019\", data_dir=\"plate-model-repo\")\n",
    "\n",
    "# Set up the PlateReconstruction model using the loaded plate model data\n",
    "# This includes rotation models, topologies, and static polygons, with the specified anchor plate ID\n",
    "model = gplately.PlateReconstruction(\n",
    "    plate_model.get_rotation_model(), \n",
    "    plate_model.get_topologies(), \n",
    "    plate_model.get_static_polygons(),\n",
    "    anchor_plate_id=anchor_plate_id\n",
    ")\n",
    "\n",
    "# Initialize the plotting object for visualizing topologies\n",
    "# The layers used for plotting include coastlines, continental polygons, and COBs (Continental Ocean Boundaries)\n",
    "gplot = gplately.plot.PlotTopologies(\n",
    "    model, \n",
    "    plate_model.get_layer('Coastlines'), \n",
    "    plate_model.get_layer('ContinentalPolygons'), \n",
    "    plate_model.get_layer('COBs')\n",
    ")\n",
    "\n",
    "# get the reconstruction of subduction zones\n",
    "subduction_data = model.tessellate_subduction_zones(reconstruction_time, \n",
    "                                                   # tessellation_threshold_radians=0.01, \n",
    "                                                    anchor_plate_id=anchor_plate_id,\n",
    "                                                    ignore_warnings=True)\n",
    "# get all the trench ids\n",
    "temp = [row[9] for row in subduction_data]\n",
    "trench_pids = sorted(set(temp))\n",
    "\n",
    "# Initialize the age grid raster, which will be used for age-related computations\n",
    "age_grid_raster = gplately.Raster(\n",
    "                                data=plate_model.get_raster(\"AgeGrids\",reconstruction_time),\n",
    "                                plate_reconstruction=model,\n",
    "                                extent=[-180, 180, -90, 90]\n",
    "                                )\n",
    "# fill Nan values, it seems to not cause any issue in interpolating the ages.\n",
    "# otherwise, there are many points where the trench point are not covered in the Raster.\n",
    "# Thus, it seems these points are just on the boundary where some other value could be filled.\n",
    "age_grid_raster.fill_NaNs(inplace=True)\n",
    "\n",
    "# parse the name lookup file\n",
    "name_lookups = ReadFile(subduction_name_lookup_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some General Utilities\n",
    "\n",
    "#### Search a Single Subduction Zone\n",
    "\n",
    "##### Search with a key word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(name_lookups[\"trench_names\"])\n",
    "\n",
    "# keyword = \"ryu\"\n",
    "\n",
    "# matching_indices = [i for i, name in enumerate(name_lookups[\"trench_names\"]) if keyword.lower() in name.lower()]\n",
    "# for index in matching_indices:\n",
    "#     print(index)\n",
    "#     print(\"name: \", name_lookups[\"trench_names\"][index])\n",
    "#     print(\"id: \", name_lookups[\"trench_pids\"][index])\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search with a trench id\n",
    "\n",
    "The following block creates a global map displaying geological features and highlights a specific subduction\n",
    "In order to do this, it first looks up in a global dataset. And then plot the one subduction zone in a global map\n",
    "\n",
    "Below are some examples of the trench ids\n",
    "\n",
    "* 12001, ,CAS\n",
    "* 686, Indonesian bndy w AUS-mg, ANDA-SUM\n",
    "* 736, Java SZ\n",
    "* 651, Flores Banda SZ, JAVA\n",
    "* 669, North Sulawesi Subduction, SULA\n",
    "* 612, Luzon subduction, LUZ\n",
    "* 678, Philippine trench, PHIL\n",
    "* 648, Okinawa Trough (Ryuku) from EarthByte cob MG 4-20-07\n",
    "* 659, Izu Bonin Trench\n",
    "* 699, Marianas Trench-NUVEL\n",
    "* 111, Aleutian and Bering Sea Masking Polygon\n",
    "* 406, Kamchatka SZ from EarthByte COB at 0Ma -- MG 4/20/07\n",
    "* 413, Shirshov Ridge Subduction (not included), part of 901 (subduction id)\n",
    "* 2000, Central American subduction from 135 Ma\n",
    "* 201, South America trench taken from a combination of RUM and COB file\n",
    "* 2031, Caribbean/farallon subduction iniating at 85 Ma\n",
    "* 2011, Caribbean subduction\n",
    "* 815, Sandwich Trench\n",
    "* 821, Tonga-Kermadec-MG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: the current plot functions result in a redundant grid around the actual plot\n",
    "# # Define the trench plate ID\n",
    "# trench_pid = 111\n",
    "# # Look up the name of the subduction zone using the trench ID\n",
    "# _name = LookupNameByPid(name_lookups[\"trench_pids\"], name_lookups[\"trench_names\"], trench_pid)\n",
    "# print(_name)\n",
    "\n",
    "# # Get the data for the specified subduction zone using the trench ID\n",
    "# # The data is returned as a pandas DataFrame\n",
    "# one_subduction_data = get_one_subduction_by_trench_id(subduction_data, trench_pid, all_columns)\n",
    "\n",
    "# # Create a new figure for plotting\n",
    "# fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "# plt.title(f'{reconstruction_time} Ma')  # Set the title to the reconstruction time\n",
    "\n",
    "# # Set up the axes with a Mollweide projection, centered at longitude 180\n",
    "# ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude=180))\n",
    "\n",
    "# # Plot global features such as coastlines and the age grid using the custom plotting function\n",
    "# ax = plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "# # Plot the subduction zone data as red scatter points\n",
    "# # Use the PlateCarree projection to correctly position the points on the map\n",
    "# im_sub = ax.scatter(one_subduction_data.lon, one_subduction_data.lat, \n",
    "#                     marker=\".\", s=3, c='r', transform=ccrs.PlateCarree())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow to extract a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Methodology\n",
    "\n",
    "In this section, we define the methodology for working with the subduction dataset.\n",
    "\n",
    "1. **Loading Data**: \n",
    "   - You can choose to either load an existing CSV file containing previously saved data or start the process from scratch.\n",
    "\n",
    "2. **Sampling Trenches**:\n",
    "   - You can choose between two sampling methods:\n",
    "     - **Sample all trenches at once**: This option allows you to sample all trenches in a single operation.\n",
    "     - **Sample a specific trench**: Alternatively, you can focus on sampling a single trench for more granular analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample by a give arc length edge and resample section\n",
    "# arc_length_edge = 0.0; arc_length_resample_section = 2.0  # by degree\n",
    "arc_length_edge = 2.0; arc_length_resample_section = 2.0  # by degree\n",
    "\n",
    "use_recorded_file = True; resample_all = True; trench_pid = None # use this option to start from a recorded file\n",
    "# use_recorded_file = False; resample_all = True; trench_pid = None # use this option to start from scratch and sample all subductions\n",
    "# use_recorded_file = False; resample_all = False; trench_pid = 2000 # use this option to start from scratch and sample one specific trench\n",
    "\n",
    "# resample the subduction zones\n",
    "\n",
    "## archived files\n",
    "# recorded_file = os.path.join(ASPECT_LAB_DIR, \"files\", \"ThDSubduction\", \"gplate_json_files\", \"subduction_resampled_t_%.2e.csv\" \\\n",
    "#                         % (reconstruction_time))\n",
    "recorded_file = os.path.join(ASPECT_LAB_DIR, \"files\", \"ThDSubduction\", \"gplate_json_files\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f_11112024.csv\" \\\n",
    "                             % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "\n",
    "## temp file\n",
    "# recorded_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.csv\" \\\n",
    "#                             % (reconstruction_time, arc_length_edge, arc_length_resample_section))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract from the dataset\n",
    "\n",
    "The following code block resamples the following dataframe:\n",
    "\n",
    "- **subduction_data_resampled**: DataFrame containing resampled subduction data, either read from a recorded file or generated by resampling all subduction zones or a specific trench ID.\n",
    "\n",
    "This contains some extra columns:\n",
    "\n",
    "- **lon_fix**: Column added to `subduction_data_resampled` to store corrected longitude values for visual or analytical adjustments, initialized with NaN values.\n",
    "- **lat_fix**: Column added to `subduction_data_resampled` to store corrected latitude values, initialized with NaN values.\n",
    "- **fix_age_polarity**: Column indicating the polarity of the age correction, helping to adjust for geological positioning, initialized with NaN values.\n",
    "- **marker**: Column used to specify marker types for visualization, initialized with NaN values.\n",
    "- **marker_fill**: Column defining the fill style of markers (e.g., 'none'), used in plotting or visualization tasks, initialized to \"none\" for all entries.\n",
    "- **color**: Column for specifying color information for each data point, primarily for visual representation, initialized with NaN values.\n",
    "- **age**: Column storing the interpolated ages for each subduction point, derived from `age_grid_raster` based on the latitude and longitude values in `subduction_data_resampled`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_recorded_file:\n",
    "    # If using a recorded file, ensure the file exists and load subduction data from it.\n",
    "    print(\"use recorded file: \", recorded_file)\n",
    "    assert(os.path.isfile(recorded_file))\n",
    "    subduction_data_resampled = pd.read_csv(recorded_file)\n",
    "else:\n",
    "    # Resample subduction data based on the specified option.\n",
    "    if resample_all:\n",
    "        # Resample all subduction zones if `resample_all` is set to True.\n",
    "        subduction_data_resampled = ResampleAllSubduction(\n",
    "            subduction_data, trench_pids, arc_length_edge, arc_length_resample_section, all_columns\n",
    "        )\n",
    "    else:\n",
    "        # Resample a specific subduction zone by trench ID if `resample_all` is False.\n",
    "        subduction_data_resampled, _ = ResampleSubductionById(\n",
    "            subduction_data, trench_pid, arc_length_edge, arc_length_resample_section, all_columns\n",
    "        )\n",
    "    \n",
    "    # Initialize new columns in the DataFrame for age correction and visualization attributes.\n",
    "    subduction_data_resampled['lon_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['lat_fix'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['fix_age_polarity'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['marker'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['marker_fill'] = ['none' for i in range(len(subduction_data_resampled))]\n",
    "    subduction_data_resampled['color'] = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "\n",
    "    subduction_data_resampled['age']  = age_grid_raster.interpolate(subduction_data_resampled.lon, subduction_data_resampled.lat, method=\"nearest\")\n",
    "\n",
    "    # Output the number of rows for debugging purposes.\n",
    "    print(\"Total resampled points: \", len(subduction_data_resampled))\n",
    "\n",
    "    # Correct invalid or missing age values in the resampled data.\n",
    "    # DEPRICATED: with the age_grid_raster.fill_NaNs, the following function is not needed anymore.\n",
    "    # The following two lines are kept for test purposes\n",
    "\n",
    "    # subduction_data_resampled['age']  = [np.nan for i in range(len(subduction_data_resampled))]\n",
    "    # FixTrenchAge(subduction_data_resampled, age_grid_raster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for invalid age values\n",
    "\n",
    "Just double check that we get the correct age values everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the invalid indexes\n",
    "invalid_indexes = []\n",
    "for i in range(len(subduction_data_resampled['age'])):\n",
    "    if np.isnan(subduction_data_resampled['age'][i]):\n",
    "        invalid_indexes.append(i)\n",
    "\n",
    "print(\"len(ages): \")\n",
    "print(len(subduction_data_resampled['age']))\n",
    "print(\"ages: \")\n",
    "print(subduction_data_resampled['age'])\n",
    "print(\"invalid_indexes: \")\n",
    "print(invalid_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix the invalid values\n",
    "\n",
    "First, map out the invalid ages from the outputs from the previous section.\n",
    "\n",
    "Use the index, theta and direction to create a new sampling point interact with the raster data\n",
    "* i_fs: the index\n",
    "* theta_fs: theta of the direction. 0 is north and 180 is south.\n",
    "* d_fs: distance along the direction\n",
    "\n",
    "The next section is for plotting the current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_fs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "# theta_fs = [180.0, 180.0, 180.0, 180.0, 180.0, 180.0, 180.0, 210.0, 210.0, 150.0, 180.0, 180.0]\n",
    "# d_fs = [100e3, 100e3, 100e3, 100e3, 100e3, 200e3, 400e3, 300e3, 200e3, 300e3, 200e3, 1000e3]\n",
    "# for ii in range(len(i_fs)):\n",
    "#     i_f = i_fs[ii]\n",
    "#     theta_f = theta_fs[ii]\n",
    "#     d_f = d_fs[ii]\n",
    "#     subduction_data_resampled_local = None\n",
    "#     subduction_data_resampled_local = pd.DataFrame([subduction_data_resampled.iloc[i_f]])\n",
    "#     subduction_data_resampled_local.lon, subduction_data_resampled_local.lat = \\\n",
    "#         Utilities.map_point_by_distance(subduction_data_resampled.iloc[i_f].lon, subduction_data_resampled.iloc[i_f].lat, theta_f, d_f)\n",
    "#     new_age = GClass.InterpolateAgeGrid(subduction_data_resampled_local)\n",
    "#     print(i_f, \": new age - \", new_age)\n",
    "#     subduction_data_resampled.loc[i_f, 'age'] = new_age\n",
    "#     if new_age is not np.nan:\n",
    "#         new_lon = subduction_data_resampled_local.lon.values[0]\n",
    "#         new_lat = subduction_data_resampled_local.lat.values[0]\n",
    "#         subduction_data_resampled.loc[i_f, 'lon_fix'] = new_lon\n",
    "#         subduction_data_resampled.loc[i_f, 'lat_fix'] = new_lat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix the dataset of Ryuku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ryuku_dataset = pd.DataFrame(np.nan, index=range(5), columns=subduction_data_resampled.columns)\n",
    "\n",
    "# ryuku_dataset['lat'] = [23.4, 24.2, 25.7, 27.5, 29.8]\n",
    "# ryuku_dataset['lon'] = [124.0, 127.0, 129.0, 130.5, 132.0]\n",
    "# ryuku_dataset['age'] = [35.0, 38.0, 48.0, 50.0, 50.0]\n",
    "# ryuku_dataset['trench_velocity']= [3.0, 0.9, 1.2, 0.7, 0.9]\n",
    "\n",
    "# subduction_data_resampled = pd.concat([subduction_data_resampled, ryuku_dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add a vector to determined whether the slab is deep enough\n",
    "\n",
    "This feature is then used when calculating the distance of one slab to another. If the slab is tagged with \"deep slab\", then it will be selected as a potential barriar in the mantle and used in the calculation of the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_deep_slab = False\n",
    "\n",
    "if not use_recorded_file:    \n",
    "    # for the first time, initiate the column in the data\n",
    "    subduction_data_resampled['deep_slab'] = [1.0 for i in range(len(subduction_data_resampled))]\n",
    "\n",
    "if add_deep_slab:\n",
    "    \n",
    "    # assign_zero_indexes = []; assign_zero_subducting_pid = None; assign_zero_trench_pid = None; exclude_fractional_pids=False # by indexed\n",
    "    # assign_zero_indexes = []; assign_zero_subducting_pid = None; assign_zero_trench_pid = None;  exclude_fractional_pids=True# by indexed\n",
    "    # assign_zero_indexes = []; assign_zero_subducting_pid = 645.0; assign_zero_trench_pid = 669.0; exclude_fractional_pids=False # by pids\n",
    "    \n",
    "    # assign_zero_indexes = []; assign_zero_subducting_pid = 801.0; assign_zero_trench_pid = 827.0; exclude_fractional_pids=False # experiment to remove north Fiji\n",
    "\n",
    "    if len(assign_zero_indexes) > 0:\n",
    "        # by directly assign by indexes\n",
    "        mask = np.zeros(len(subduction_data_resampled), dtype=bool)\n",
    "        for i in assign_zero_indexes:\n",
    "            mask[i] = 1\n",
    "    elif assign_zero_subducting_pid is not None:\n",
    "        # by subducting_pid and trench_pid\n",
    "        mask = mask_by_pids(subduction_data_resampled, assign_zero_subducting_pid, assign_zero_trench_pid)\n",
    "    elif exclude_fractional_pids:\n",
    "        # by excluding the fractional pid numbers\n",
    "        mask = (abs(subduction_data_resampled[\"subducting_pid\"] - np.round(subduction_data_resampled[\"subducting_pid\"])) > 0.1)\n",
    "    else:\n",
    "        mask = np.zeros(len(subduction_data_resampled), dtype=bool)\n",
    "\n",
    "    subduction_data_resampled.loc[mask, \"deep_slab\"] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the dataset\n",
    "\n",
    "##### View the whole dataset or the seleted points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic plots\n",
    "\n",
    "plot_whole_dataset = True\n",
    "# plot the reconstructed zone\n",
    "# i_p = None; subducting_pid_p = None; trench_pid_p = None; # plot all\n",
    "# i_p = 76; subducting_pid_p = None; trench_pid_p = None; # plot one point\n",
    "i_p = None; subducting_pid_p = 608.0; trench_pid_p = 737.0; # plot one subduction zone by pid lookup\n",
    "\n",
    "if plot_whole_dataset:\n",
    "\n",
    "    if i_p is not None:\n",
    "        mask = np.zeros(len(subduction_data_resampled), dtype=bool)\n",
    "        mask[i_p] = 1\n",
    "    elif subducting_pid_p is not None:\n",
    "        mask = mask_by_pids(subduction_data_resampled, subducting_pid_p, trench_pid_p)\n",
    "        # print the name of subduction zone\n",
    "        _name = LookupNameByPid(name_lookups[\"trench_pids\"], name_lookups[\"trench_names\"],\\\n",
    "            int(np.round(trench_pid_p)))\n",
    "        print(_name)\n",
    "    else:\n",
    "        mask = np.ones(len(subduction_data_resampled), dtype=bool)\n",
    "            \n",
    "\n",
    "    fig = plt.figure(figsize=(10,6), dpi=100)\n",
    "    ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude = 180))\n",
    "\n",
    "    # Plot global features such as coastlines and the age grid using the custom plotting function\n",
    "    ax = plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "    # debug\n",
    "    # print(subduction_data_resampled['age'])\n",
    "\n",
    "    # plot all the fixed ages\n",
    "    # mask = (~subduction_data_resampled['age'].isna())\n",
    "    \n",
    "    # plot all points\n",
    "    ax.scatter(subduction_data_resampled[mask].lon, subduction_data_resampled[mask].lat, marker=\".\", s=60, c='r', transform=ccrs.PlateCarree())\n",
    "    # ax.scatter(subduction_data_resampled[~mask].lon, subduction_data_resampled[~mask].lat, marker=\".\", s=60, c='y', transform=ccrs.PlateCarree())\n",
    "    ax.scatter(subduction_data_resampled[mask].lon_fix, subduction_data_resampled[mask].lat_fix, marker=\".\", s=30, c='c', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # write outputs\n",
    "    # fileout = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.pdf\" \\\n",
    "    #                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "    # fileout1 = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.png\" \\\n",
    "    #                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "    # if not (os.path.isdir(os.path.dirname(fileout))):\n",
    "    #     os.mkdir(os.path.dirname(fileout))\n",
    "    # fig.savefig(fileout)\n",
    "    # fig.savefig(fileout1)\n",
    "    # print(\"Save figure: %s\" % fileout)\n",
    "    # print(\"Save figure: %s\" % fileout1)\n",
    "\n",
    "else:\n",
    "    print(\"Skip plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select and plot the points based on nearness to a query point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_near_points = False\n",
    "# plot the reconstructed zone\n",
    "# query longitude, latitude and distance\n",
    "lat_q = -14.0\n",
    "lon_q = 165.0\n",
    "dist_q = 500 # km\n",
    "\n",
    "n_near_points = 0\n",
    "point_indexes = []\n",
    "if plot_near_points:\n",
    "\n",
    "    mask = np.zeros(len(subduction_data_resampled), dtype=bool)\n",
    "    for i in range(len(subduction_data_resampled)):\n",
    "        dist = haversine(lat_q, lon_q, subduction_data_resampled.lat[i], subduction_data_resampled.lon[i], radius=6371)\n",
    "        if dist < dist_q:\n",
    "            mask[i] = 1\n",
    "            n_near_points += 1\n",
    "            point_indexes.append(i)\n",
    "\n",
    "    # print the near points, subducting and trench pid, as well as trench name\n",
    "    subduction_data_resampled_masked = subduction_data_resampled[mask]\n",
    "    for i in range(n_near_points):\n",
    "        _name = LookupNameByPid(name_lookups[\"trench_pids\"], name_lookups[\"trench_names\"],\\\n",
    "            int(np.round(subduction_data_resampled_masked.trench_pid[point_indexes[i]])))\n",
    "        print(\"point indexes\\t\", \"lat\\t\\t\\t\" ,\"lon\\t\\t\\t\", \"subducting_pid\\t\", \"trench_pid\\t\", \"name\\t\")\n",
    "        print(point_indexes[i], \"\\t\\t\", subduction_data_resampled_masked.lat[point_indexes[i]], \"\\t\",\\\n",
    "              subduction_data_resampled_masked.lon[point_indexes[i]], \"\\t\",\\\n",
    "              subduction_data_resampled_masked.subducting_pid[point_indexes[i]],\"\\t\\t\",\\\n",
    "              subduction_data_resampled_masked.trench_pid[point_indexes[i]],\"\\t\\t\", _name,\"\\t\")\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6), dpi=100)\n",
    "    ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude = 180))\n",
    "\n",
    "    # Plot global features such as coastlines and the age grid using the custom plotting function\n",
    "    ax = plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "    # # debug\n",
    "    # print(subduction_data_resampled['age'])\n",
    "\n",
    "    # # plot all the fixed ages\n",
    "    # mask = (~subduction_data_resampled['age'].isna())\n",
    "    \n",
    "    # plot all points\n",
    "    ax.scatter(lon_q, lat_q, marker=\".\", s=200, c='purple', transform=ccrs.PlateCarree(), label=\"Query point\")\n",
    "    ax.scatter(subduction_data_resampled[mask].lon, subduction_data_resampled[mask].lat, marker=\".\", s=60, c='r', transform=ccrs.PlateCarree(), label=\"Near points\")\n",
    "    # ax.scatter(subduction_data_resampled[~mask].lon, subduction_data_resampled[~mask].lat, marker=\".\", s=60, c='y', transform=ccrs.PlateCarree())\n",
    "    # ax.scatter(subduction_data_resampled[mask].lon_fix, subduction_data_resampled[mask].lat_fix, marker=\".\", s=30, c='c', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # write outputs\n",
    "    # fileout = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.pdf\" \\\n",
    "    #                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "    # fileout1 = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.png\" \\\n",
    "    #                         % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "    # if not (os.path.isdir(os.path.dirname(fileout))):\n",
    "    #     os.mkdir(os.path.dirname(fileout))\n",
    "    # fig.savefig(fileout)\n",
    "    # fig.savefig(fileout1)\n",
    "    # print(\"Save figure: %s\" % fileout)\n",
    "    # print(\"Save figure: %s\" % fileout1)\n",
    "\n",
    "else:\n",
    "    print(\"Skip plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_file = False\n",
    "# export the file to a temp file\n",
    "if resample_all:\n",
    "    temp_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_edge_%.2f_section_%.2f.csv\" \\\n",
    "                            % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "else:\n",
    "    temp_file = os.path.join(ASPECT_LAB_DIR, \"dtemp\", \"gplate_export_test0\", \"subduction_resampled_t_%.2e_pid_%d_edge_%.2f_section_%.2f.csv\" \\\n",
    "                         % (reconstruction_time, int(trench_pid), arc_length_edge, arc_length_resample_section))\n",
    "\n",
    "# don't mess up the existing files\n",
    "if record_file:\n",
    "    if use_recorded_file:\n",
    "        # in case we read from an existing file, omit the index when output\n",
    "        subduction_data_resampled.to_csv(temp_file, index=False)\n",
    "    else:\n",
    "        subduction_data_resampled.to_csv(temp_file)\n",
    "\n",
    "    print(\"Data saved to %s\" % temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze extracted data set\n",
    "\n",
    "We plot:\n",
    "\n",
    "* age vs convergence / trench retreat rate of trenches below.\n",
    "* trench distance vs convergence / trench retreat rate of trenches below.\n",
    "\n",
    "#### Some notes\n",
    "\n",
    "##### On the Proccedure\n",
    "\n",
    "- Filtering short slab using the \"deep_slab\" vector would vary the result quite a bit.\n",
    "\n",
    "##### On specific subduction zones\n",
    "\n",
    "* TON-KERM: by calculating the distance, points are matched to the other side of north Fiji Basin, resulting in small distance value.\n",
    "* Ryuku: data is missing in the current dataset, fixing using values form Table 1 in Lallemend et al., 2005\n",
    "* JAVA: One single point has an advance motion.\n",
    "* LUZ: gplately data suggest advance of 2.5, while L05 shows fast retreat motion. When computing the distance, the LUZ will be matched to the east LUZ and result in very small values\n",
    "* KuKam: data shows retreat motion, while L05 shows advance motion\n",
    "* PER-NCHI-JUAN-SCHI (subduction id 911): These seem to be all in the subduction id 901. Beyond a trench id of 201, the trench id varies from 201010 to 20101?. Up north, the component is marked with trench id 2031 (Caribbean/farallon subduction iniating at 85 Ma).\n",
    "* South to SCHI: This is represented by the subduction id 801, but the trench id 201 continues from the previous 901 subduction.\n",
    "* ANT: retreat motion in this dataset, rather than the advance motion in L05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate a distance to an adjacent subduction zone\n",
    "\n",
    "**Find Nearest Deep Subduction Points**:\n",
    "   - For each subduction point, iterates over all other points in the dataset.\n",
    "   - Filters out shallow subduction points and those within the same subduction zone.\n",
    "   - Calculates the distance and bearing angle to each other point, checking if it falls within the specified angle tolerance (`theta_diff`).\n",
    "   - Updates the minimum distance if a closer deep subduction point is found within the angle tolerance (initialized as the length of the Earth's equator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Geod\n",
    "\n",
    "# Define Earth's radius in meters\n",
    "Ro = 6371e3\n",
    "\n",
    "# Initialize the Geod object with the WGS84 ellipsoid model, \n",
    "# which provides accurate geodesic calculations on Earth's surface\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Initialize arrays to store distances, indexes, and markers for subduction data points\n",
    "subduction_distances = np.zeros(len(subduction_data_resampled))  # Stores the minimum distance for each subduction point\n",
    "subduction_indexes = np.array(range(len(subduction_data_resampled)))  # Indexes for each point in the dataset\n",
    "subduction_min_distance_indexes = np.full(len(subduction_data_resampled), -1, dtype=int)  # Indexes of the closest deep subduction point\n",
    "subduction_maker_lons = []  # Stores longitudes of marker points\n",
    "subduction_maker_lats = []  # Stores latitudes of marker points\n",
    "\n",
    "# Define the default comparison distance as Earth's circumference\n",
    "# and set parameters for marker distance and angle tolerance\n",
    "d_m = 1000e3  # Distance of the marker point in meters\n",
    "theta_diff = 45.0  # Tolerance angle in degrees for direction comparison\n",
    "\n",
    "# Iterate over each subduction point in the resampled data to compute closest distances\n",
    "for i_1 in range(len(subduction_data_resampled)):\n",
    "    subducting_pid_1 = int(subduction_data_resampled.subducting_pid[i_1])\n",
    "    lat1, lon1 = subduction_data_resampled.lat[i_1], subduction_data_resampled.lon[i_1]\n",
    "\n",
    "    # Place a marker point at a distance `d_m` along the trench normal angle (`theta`)\n",
    "    theta = subduction_data_resampled.trench_azimuth_angle[i_1]\n",
    "    theta = (theta + 360) % 360  # Normalize angle `theta` to the range [0, 360)\n",
    "    lon_m, lat_m = Utilities.map_point_by_distance(lon1, lat1, theta, d_m)\n",
    "\n",
    "    # Get the path coordinates between the original point and marker point\n",
    "    path_lon_m, path_lat_m = Utilities.shortest_path_between_two_points([lon1, lat1], [lon_m, lat_m], geod.npts, 100)\n",
    "    subduction_maker_lons.append(path_lon_m)\n",
    "    subduction_maker_lats.append(path_lat_m)\n",
    "\n",
    "    # Set initial comparison distance to Earth's circumference\n",
    "    distance = 2 * np.pi * Ro\n",
    "\n",
    "    # Find the closest deep subduction point with matching angle tolerance\n",
    "    for j_2 in range(len(subduction_data_resampled)):\n",
    "        subducting_pid_2 = int(subduction_data_resampled.subducting_pid[j_2])\n",
    "\n",
    "        # Skip shallow slabs (< 410 km depth)\n",
    "        if not subduction_data_resampled.deep_slab[j_2]:\n",
    "            continue\n",
    "        \n",
    "        # Skip points on the same subduction zone (including itself)\n",
    "        if subducting_pid_1 == subducting_pid_2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the distance and bearing to the other point\n",
    "        lat2, lon2 = subduction_data_resampled.lat[j_2], subduction_data_resampled.lon[j_2]\n",
    "        theta_1 = Utilities.calculate_bearing(lon1, lat1, lon2, lat2)  # Bearing angle to the other point\n",
    "        distance_between_points = haversine(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "        # Update distance if closer point is found within the angle tolerance\n",
    "        if distance_between_points < distance and abs(theta_1 - theta) < theta_diff:\n",
    "            distance = distance_between_points\n",
    "            subduction_min_distance_indexes[i_1] = j_2\n",
    "    \n",
    "    # Store the computed minimum distance for the current point\n",
    "    subduction_distances[i_1] = distance\n",
    "\n",
    "# Add the computed distances to the DataFrame as a new column\n",
    "subduction_data_resampled[\"near_distance\"] = subduction_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the matching points of minimum distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting parameters to specify which subduction zone or point to display\n",
    "i_p = None; subducting_pid = None; trench_pid=None # Uncomment to plot one subduction zone\n",
    "# i_p = 43; subducting_pid = None; trench_pid=None # Uncomment to plot one point\n",
    "# i_p = None; subducting_pid = 801; trench_pid = None  # Plot specific subduction zone with subducting plate ID\n",
    "\n",
    "# Initialize the figure and axis with a Mollweide projection for global plotting\n",
    "fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "ax = fig.add_subplot(111, projection=ccrs.Mollweide(central_longitude=180))\n",
    "\n",
    "# Plot the global coastline, age grid, and other basic features on the map\n",
    "plot_global_basics(ax, gplot, age_grid_raster, reconstruction_time)\n",
    "\n",
    "# Create masks to filter subduction points based on specified parameters\n",
    "mask1 = MaskBySubductionTrenchIds(subduction_data_resampled, subducting_pid, trench_pid, i_p)  # Filter by IDs\n",
    "mask2 = (subduction_min_distance_indexes >= 0)  # Filter points with valid trench distances\n",
    "\n",
    "# Combine masks to get points with and without valid trench distances\n",
    "mask = mask1 & mask2  # Valid points with trench distance\n",
    "mask_in = mask1 & (~mask2)  # Invalid points without trench distance\n",
    "\n",
    "# Separate valid and invalid indexes for plotting\n",
    "min_indexes_invalid = subduction_min_distance_indexes[mask_in]\n",
    "min_indexes_valid = subduction_min_distance_indexes[mask]\n",
    "indexes_valid = subduction_indexes[mask]\n",
    "\n",
    "# Plot pairs of matching points in the subduction zones\n",
    "# This includes plotting each marker path along the convergence direction and distance `d_m`\n",
    "ax.scatter(subduction_data_resampled.lon[mask], subduction_data_resampled.lat[mask], \n",
    "           marker=\".\", s=60, c='r', transform=ccrs.PlateCarree())  # Plot valid points in red\n",
    "ax.scatter(subduction_data_resampled.lon[mask_in], subduction_data_resampled.lat[mask_in], \n",
    "           marker=\".\", s=60, c='purple', transform=ccrs.PlateCarree())  # Plot invalid points in purple\n",
    "\n",
    "# Loop through each valid pair of points to plot their matching markers and connection lines\n",
    "for i in range(len(min_indexes_valid)):\n",
    "    try:\n",
    "        # Plot the matching points in blue\n",
    "        ax.scatter(subduction_data_resampled.lon[min_indexes_valid[i]], \n",
    "                   subduction_data_resampled.lat[min_indexes_valid[i]], \n",
    "                   marker=\".\", s=60, c='b', transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # Draw a line between each query point and its matching subduction point\n",
    "        ax.plot([subduction_data_resampled.lon[indexes_valid[i]], subduction_data_resampled.lon[min_indexes_valid[i]]],\n",
    "                [subduction_data_resampled.lat[indexes_valid[i]], subduction_data_resampled.lat[min_indexes_valid[i]]],\n",
    "                c='c', transform=ccrs.PlateCarree())  # Plot lines in cyan\n",
    "    except KeyError:\n",
    "        pass  # Skip plotting if an index is missing (for debugging purposes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary keys represent subduction zone IDs, and the values specify\n",
    "\n",
    "from matplotlib.path import Path\n",
    "# the marker style, face color, and name associated with that ID.\n",
    "# this definition of snowflake initially has an error in the \"code\" part\n",
    "verts = [\n",
    "    (0., 0.),   # Center\n",
    "    (0.2, 0.6), # Upper arm\n",
    "    (0., 0.),   # Center\n",
    "    (0.4, 0.4), # Right diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (0.6, 0.2), # Right arm\n",
    "    (0., 0.),   # Center\n",
    "    (0.4, -0.4),# Right down diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (0.2, -0.6),# Bottom arm\n",
    "    (0., 0.),   # Center\n",
    "    (-0.4, -0.4),# Left down diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (-0.6, -0.2),# Left arm\n",
    "    (0., 0.),   # Center\n",
    "    (-0.4, 0.4),# Left diagonal\n",
    "    (0., 0.),   # Center\n",
    "    (-0.2, 0.6),# Upper left arm\n",
    "]\n",
    "codes = [Path.MOVETO] + [Path.LINETO, Path.MOVETO] * 8 + [Path.MOVETO]\n",
    "snowflake = Path(verts, codes)\n",
    "\n",
    "# Define vertices for two equilateral triangles\n",
    "vertices = [\n",
    "    [0, 1], [-np.sqrt(3)/2, -0.5], [np.sqrt(3)/2, -0.5], [0, 1],  # First triangle\n",
    "    [0, -1], [-np.sqrt(3)/2, 0.5], [np.sqrt(3)/2, 0.5], [0, -1]   # Second triangle\n",
    "]\n",
    "# Flatten the vertices list for creating the Path\n",
    "vertices = np.array(vertices)\n",
    "# Define path codes (all 'LINETO' except the start 'MOVETO')\n",
    "codes = [Path.MOVETO] + [Path.LINETO] * (len(vertices) - 1)\n",
    "\n",
    "star_path = Path(vertices, codes)\n",
    "\n",
    "plot_options = \\\n",
    "{\n",
    "    903: {\"marker\": 'o',  \"markerfacecolor\": \"yellow\", \"name\": \"CAS\"},\n",
    "    511: {\"marker\": 's',  \"markerfacecolor\": \"yellow\", \"name\": \"ANDA-SUM\"},\n",
    "    801: {\"marker\": 'd',  \"markerfacecolor\": \"yellow\", \"name\": \"JAVA\"},\n",
    "    645: {\"marker\": snowflake,  \"markerfacecolor\": \"black\", \"name\": \"SULA\"},\n",
    "    602: {\"marker\": 'x',  \"markerfacecolor\": \"blue\", \"name\": \"LUZ\"},\n",
    "    608: {\"marker\": 's',  \"markerfacecolor\": 'c', \"name\": \"PHIL\"},\n",
    "    901: {\n",
    "        699: {\"marker\": '>',  \"markerfacecolor\": 'red', \"name\": \"MAR\"},\n",
    "        659: {\"marker\": 's',  \"markerfacecolor\": 'red', \"name\": \"IZU\"},\n",
    "        (601112.0, 601118.0): {\"marker\": '^',  \"markerfacecolor\": 'green', \"name\": \"JAP\"},\n",
    "        406:{\"marker\": 'v',  \"markerfacecolor\": 'green', \"name\": \"KUKAM\"},\n",
    "        111: {\"marker\": 'o',  \"markerfacecolor\": 'pink', \"name\": \"ALE-ALA\"},\n",
    "        (806, 821): {\"marker\": 'd',  \"markerfacecolor\": 'blue', \"name\": \"TON-KERM\"}\n",
    "        },\n",
    "    909: {\"marker\": star_path,  \"markerfacecolor\": 'c', \"name\": \"MEX\"},\n",
    "    911: {\"marker\": 'o',  \"markerfacecolor\": 'k', \"name\": \"PER-NCHI-JUAN-SCHI\"},\n",
    "    802: {\"marker\": 'd',  \"markerfacecolor\": 'k', \"name\": \"SSCHI-TBD\"},\n",
    "    201: {\n",
    "        2011:{\"marker\": '+',  \"markerfacecolor\": 'pink', \"name\": \"ANT\"},\n",
    "        815:{\"marker\": '*',  \"markerfacecolor\": 'r', \"name\": \"SAND\"}\n",
    "        },\n",
    "    1: {\"marker\": 'd',  \"markerfacecolor\": \"r\", \"name\": \"RYU\"}\n",
    "}\n",
    "\n",
    "# Create a figure and two subplots for plotting trench velocity data.\n",
    "# `gs` defines a 2x1 grid layout for the subplots.\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "gs = gridspec.GridSpec(3, 1)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax3 = fig.add_subplot(gs[2, 0])\n",
    "total_points_plotted = 0  # Variable to record the total number of plotted points.\n",
    "\n",
    "# Filter out rows with NaN values in the \"age\" column.\n",
    "mask_age = (~subduction_data_resampled[\"age\"].isna())\n",
    "total_points_plotted += len(subduction_data_resampled[mask_age])  # Count valid points.\n",
    "subduction_data_resampled_valid = subduction_data_resampled[mask_age]  # Store valid data.\n",
    "\n",
    "# Obtain a sorted list of unique subducting plate IDs from the valid data.\n",
    "# and round to nearest integar value\n",
    "unique_subducting_pids_0 = subduction_data_resampled_valid.subducting_pid.unique()\n",
    "\n",
    "unique_subducting_pids = []\n",
    "seen = set()\n",
    "for val in unique_subducting_pids_0:\n",
    "    if abs(val - round(val) < 0.1) and round(val) not in seen:\n",
    "           unique_subducting_pids.append(float(round(val)))\n",
    "           seen.add(round(val))\n",
    "    \n",
    "labels = []\n",
    "patches = []\n",
    "unique_subducting_pids.sort()  # Sort the unique subducting plate IDs.\n",
    "print(unique_subducting_pids)\n",
    "\n",
    "# Lookup and store subducting plate names based on their IDs.\n",
    "unique_subducting_names = []\n",
    "for i in range(len(unique_subducting_pids)):\n",
    "    subducting_pid = unique_subducting_pids[i]\n",
    "    # unique_subducting_names.append(GParseReconstruction.LookupNameByPid(int(subducting_id)))\n",
    "    unique_subducting_names.append(LookupNameByPid(trench_pids, name_lookups[\"trench_names\"], int(subducting_pid)))\n",
    "\n",
    "# Loop through each subducting plate ID and plot the corresponding trench velocity.\n",
    "k = 0\n",
    "for i in range(len(unique_subducting_pids)):\n",
    "    _name = unique_subducting_names[i]\n",
    "\n",
    "    subducting_id = unique_subducting_pids[i]\n",
    "    try:\n",
    "        plot_option_sub_dict = plot_options[int(subducting_id)]  # Get plot options for the ID.\n",
    "    except KeyError:\n",
    "        # If no specific plot option is found, use default settings.\n",
    "        continue # comment this to plot as TBD points\n",
    "        print(\"Id %s not found, marked as TBD\" % int(subducting_id))\n",
    "        plot_option_sub_dict = {\"marker\": 'o',  \"markerfacecolor\": None, \"name\": \"TBD\"}\n",
    "\n",
    "    # Make an output for the plotting function to loop over the trench ids\n",
    "    plot_trench_pids = None; plot_option_list = None\n",
    "    if 'name' in plot_option_sub_dict:\n",
    "        # A subduction contains a single trench\n",
    "        plot_trench_pids = [None]\n",
    "        plot_option_list = [plot_option_sub_dict.copy()]\n",
    "    else:\n",
    "        # A subduction contains multiple trenches\n",
    "        plot_trench_pids = []\n",
    "        plot_option_list = []\n",
    "        for key, value in plot_option_sub_dict.items():\n",
    "            plot_trench_pids.append(key)\n",
    "            plot_option_list.append(value.copy())\n",
    "\n",
    "    # Loop over the trench ids and plot the markers\n",
    "    for i_tr in range(len(plot_trench_pids)):\n",
    "        trench_pid = plot_trench_pids[i_tr]\n",
    "        plot_option = plot_option_list[i_tr]\n",
    "        # We want trench_pid options to be flexible.\n",
    "        # It could be a - a value; b - a range and c - multiple values\n",
    "        # d - None\n",
    "        # Create a mask for the current subducting plate and plot its trench velocity.\n",
    "        # We allow a variation of 0.1 from the integar value\n",
    "        # mask1 - match the subducting id\n",
    "        # mask2 - match the trench pid condition.\n",
    "        mask1 = (abs(subduction_data_resampled.subducting_pid - subducting_id) < 0.1)\n",
    "        if trench_pid is None:\n",
    "            mask = mask1\n",
    "        elif type(trench_pid) == float or type(trench_pid) == int:\n",
    "            mask = mask1 & (abs(subduction_data_resampled.trench_pid - trench_pid) < 0.1)\n",
    "        elif type(trench_pid) == list:\n",
    "            # mutiple values\n",
    "            mask2 = (abs(subduction_data_resampled.trench_pid - trench_pid[0]) < 0.1)\n",
    "            for trench_sub_pid in trench_pid[1:]:\n",
    "                mask2 = mask2 | (abs(subduction_data_resampled.trench_pid - trench_sub_pid) < 0.1)\n",
    "            mask = mask1 & mask2\n",
    "        elif type(trench_pid) == tuple:\n",
    "            # a range\n",
    "            assert(len(trench_pid) == 2)\n",
    "            mask2 = ((subduction_data_resampled.trench_pid >= trench_pid[0]) & (subduction_data_resampled.trench_pid <= trench_pid[1]))\n",
    "            mask = mask1 & mask2\n",
    "        else:\n",
    "            raise ValueError(\"Type of trench pid is wrong. Possible types are [None, float, int, list, dict]\")\n",
    "        ages = subduction_data_resampled_valid.age[mask]\n",
    "        trench_velocities = subduction_data_resampled_valid.trench_velocity[mask]\n",
    "        near_distances = subduction_data_resampled_valid.near_distance[mask]\n",
    "        _patch = ax1.plot(ages, trench_velocities,\\\n",
    "                marker=plot_option[\"marker\"], markerfacecolor=plot_option[\"markerfacecolor\"],\\\n",
    "                markeredgecolor='black', markersize=10, linestyle='', label=plot_option[\"name\"])[0]\n",
    "        _patch_d = ax3.plot(near_distances, trench_velocities,\\\n",
    "                marker=plot_option[\"marker\"], markerfacecolor=plot_option[\"markerfacecolor\"],\\\n",
    "                markeredgecolor='black', markersize=10, linestyle='', label=plot_option[\"name\"])[0]\n",
    "        patches.append(_patch)\n",
    "        print(k, \", subducting_id = \", subducting_id, \", trench_pid = \", trench_pid)\n",
    "        k += 1\n",
    "\n",
    "# Configure grid and legend for the second subplot.\n",
    "ax1.grid()\n",
    "ax3.grid()\n",
    "ax2.legend(handles=patches, bbox_to_anchor=(0.5, 0.5), loc='center', ncol=2, numpoints=1, frameon=False)\n",
    "\n",
    "# Output the total number of plotted points.\n",
    "print(\"Total plotted points: %d\" % total_points_plotted)\n",
    "\n",
    "# Set axis limits and labels for the first plot (trench velocity vs age).\n",
    "ax1.set_xlim([0, 160.0])\n",
    "ax1.set_ylim([-10.0, 10.0])\n",
    "ax3.set_ylim([-10.0, 10.0])\n",
    "ax1.set_xlabel(\"Age (Ma)\")\n",
    "ax1.set_ylabel(\"Trench Velocity Magnitude (cm/yr)\")\n",
    "ax3.set_ylabel(\"Trench Velocity Magnitude (cm/yr)\")\n",
    "\n",
    "# Save the figure to a PDF file with a name derived from the reconstruction parameters.\n",
    "fileout = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_distribution_t_%.2e_edge_%.2f_section_%.2f.pdf\"\\\n",
    "     % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "fig.savefig(fileout)\n",
    "print(\"figure saved: %s\" % fileout)\n",
    "\n",
    "# Save the subducting plate ID and names to a CSV file for future reference.\n",
    "csv_out = os.path.join(RESULT_DIR, \"gplate_subduction_zones\", \"subduction_distribution_t_%.2e_edge_%.2f_section_%.2f.csv\"\\\n",
    "     % (reconstruction_time, arc_length_edge, arc_length_resample_section))\n",
    "unique_data = {\n",
    "    \"pid\": unique_subducting_pids,\n",
    "    'name': unique_subducting_names\n",
    "}\n",
    "df_unique_data = pd.DataFrame(unique_data)\n",
    "df_unique_data.to_csv(csv_out)\n",
    "print(\"csv file saved: %s\" % csv_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-gplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
